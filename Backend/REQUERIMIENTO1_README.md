# üì• Requerimiento 1: Automatizaci√≥n de Descarga de Datos

## üéØ Objetivo

Automatizar la descarga de publicaciones cient√≠ficas desde m√∫ltiples bases de datos, unificar formatos bibliogr√°ficos, eliminar duplicados autom√°ticamente y generar reportes detallados.

---

## ‚ú® Caracter√≠sticas Implementadas

### 1. **Descarga Automatizada Multi-Fuente** ‚úÖ
- ‚úì Scraper para CrossRef API (implementado y funcional)
- ‚è≥ Scraper para ACM Digital Library (pendiente)
- ‚è≥ Scraper para SAGE Publications (pendiente)
- ‚è≥ Scraper para ScienceDirect (pendiente)

### 2. **Unificaci√≥n de Formatos** ‚úÖ
- ‚úì Modelo de datos unificado (`Publication`)
- ‚úì Parser para respuestas JSON de APIs
- ‚úì Normalizaci√≥n de metadatos
- ‚úì Validaci√≥n con Pydantic

### 3. **Deduplicaci√≥n Inteligente** ‚úÖ
- ‚úì Comparaci√≥n por DOI (100% confiable)
- ‚úì Hash MD5 de t√≠tulos normalizados
- ‚úì Fuzzy matching de t√≠tulos (Levenshtein)
- ‚úì Umbral de similitud configurable (default: 95%)
- ‚úì Generaci√≥n de reporte detallado de duplicados

### 4. **Exportaci√≥n a M√∫ltiples Formatos** ‚úÖ
- ‚úì JSON (estructurado con todos los metadatos)
- ‚úì BibTeX (para gestores bibliogr√°ficos)
- ‚úì RIS (formato est√°ndar de referencias)
- ‚úì CSV (para an√°lisis en Excel/Python)

### 5. **API REST Completa** ‚úÖ
- ‚úì Endpoints para iniciar descarga
- ‚úì Consulta de estado de jobs
- ‚úì Descarga de resultados
- ‚úì Acceso a reporte de duplicados

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    API REST (FastAPI)                        ‚îÇ
‚îÇ                  POST /api/v1/data/download                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              UnifiedDownloader (Orquestador)                 ‚îÇ
‚îÇ  - Gesti√≥n de jobs                                          ‚îÇ
‚îÇ  - Descarga paralela                                        ‚îÇ
‚îÇ  - Progreso en tiempo real                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ              ‚îÇ              ‚îÇ
             ‚ñº              ‚ñº              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  CrossRef  ‚îÇ  ‚îÇ    ACM     ‚îÇ  ‚îÇ    SAGE    ‚îÇ
    ‚îÇ  Scraper   ‚îÇ  ‚îÇ  Scraper   ‚îÇ  ‚îÇ  Scraper   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ               ‚îÇ               ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      Deduplicator             ‚îÇ
         ‚îÇ  - Comparaci√≥n por DOI        ‚îÇ
         ‚îÇ  - Hash de t√≠tulos            ‚îÇ
         ‚îÇ  - Fuzzy matching             ‚îÇ
         ‚îÇ  - Reporte de duplicados      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ    Exportaci√≥n de Resultados   ‚îÇ
         ‚îÇ  - JSON                        ‚îÇ
         ‚îÇ  - BibTeX                      ‚îÇ
         ‚îÇ  - RIS                         ‚îÇ
         ‚îÇ  - CSV                         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Estructura de Archivos

```
Backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ publication.py          # Modelo unificado de publicaci√≥n
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_acquisition/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base_scraper.py     # Clase base abstracta
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ crossref_scraper.py # Scraper de CrossRef ‚úì
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ deduplicator.py     # Sistema de deduplicaci√≥n ‚úì
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ unified_downloader.py # Orquestador ‚úì
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îî‚îÄ‚îÄ v1/
‚îÇ           ‚îî‚îÄ‚îÄ data_acquisition.py # Endpoints REST ‚úì
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ downloads/                  # Archivos generados
‚îÇ       ‚îú‚îÄ‚îÄ job_xxx_unified.json
‚îÇ       ‚îú‚îÄ‚îÄ job_xxx_unified.bib
‚îÇ       ‚îú‚îÄ‚îÄ job_xxx_unified.ris
‚îÇ       ‚îú‚îÄ‚îÄ job_xxx_unified.csv
‚îÇ       ‚îú‚îÄ‚îÄ job_xxx_duplicates.json
‚îÇ       ‚îî‚îÄ‚îÄ job_xxx_summary.json
‚îÇ
‚îî‚îÄ‚îÄ test_requerimiento1.py         # Script de prueba ‚úì
```

---

## üöÄ Uso

### 1. **Ejecutar el Backend**

```bash
cd Backend
python main.py
```

El servidor estar√° disponible en: `http://localhost:8000`

### 2. **Ejecutar Script de Prueba**

```bash
cd Backend
python test_requerimiento1.py
```

### 3. **Usar la API REST**

#### a) Iniciar Descarga

```bash
curl -X POST "http://localhost:8000/api/v1/data/download" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "generative artificial intelligence",
    "sources": ["crossref"],
    "max_results_per_source": 50,
    "start_year": 2023,
    "export_formats": ["json", "bibtex"]
  }'
```

**Respuesta:**
```json
{
  "success": true,
  "job_id": "job_abc123def456",
  "status": "running",
  "message": "Descarga iniciada para query: 'generative artificial intelligence'"
}
```

#### b) Consultar Estado

```bash
curl "http://localhost:8000/api/v1/data/status/job_abc123def456"
```

**Respuesta:**
```json
{
  "job_id": "job_abc123def456",
  "query": "generative artificial intelligence",
  "sources": ["crossref"],
  "status": "completed",
  "progress": 100.0,
  "total_downloaded": 50,
  "total_unique": 45,
  "total_duplicates": 5,
  "started_at": "2025-10-20T10:00:00",
  "completed_at": "2025-10-20T10:02:30",
  "errors": []
}
```

#### c) Descargar Resultados

```bash
# Formato JSON
curl -O "http://localhost:8000/api/v1/data/download/job_abc123def456/json"

# Formato BibTeX
curl -O "http://localhost:8000/api/v1/data/download/job_abc123def456/bibtex"

# Formato CSV
curl -O "http://localhost:8000/api/v1/data/download/job_abc123def456/csv"
```

#### d) Obtener Reporte de Duplicados

```bash
curl "http://localhost:8000/api/v1/data/duplicates/job_abc123def456"
```

---

## üìä Resultados Esperados

### Estad√≠sticas T√≠picas (50 publicaciones)

```
üì• Descarga:
   - Total descargados: 50
   - crossref: 50 publicaciones

üîç Deduplicaci√≥n:
   - Publicaciones √∫nicas: 45
   - Duplicados eliminados: 5
   - Tasa de duplicaci√≥n: 10%

üìã Reporte de Duplicados:
   - Por DOI id√©ntico: 3
   - Por similitud de t√≠tulo: 2
   - Por hash: 0

üíæ Archivos generados:
   ‚úì job_xxx_unified.json       (datos completos)
   ‚úì job_xxx_unified.bib        (para Zotero/Mendeley)
   ‚úì job_xxx_unified.ris        (formato est√°ndar)
   ‚úì job_xxx_unified.csv        (para Excel)
   ‚úì job_xxx_duplicates.json    (reporte detallado)
   ‚úì job_xxx_summary.json       (resumen del job)
```

---

## üî¨ Algoritmo de Deduplicaci√≥n

### Estrategia Multi-Nivel

```python
Para cada publicaci√≥n P en lista:
    
    # Nivel 1: Comparaci√≥n exacta por DOI (O(1))
    Si P.doi existe en √≠ndice_doi:
        Marcar como duplicado
        Continuar
    
    # Nivel 2: Hash de t√≠tulo normalizado (O(1))
    hash = MD5(normalize(P.title))
    Si hash existe en √≠ndice_hash:
        Marcar como duplicado
        Continuar
    
    # Nivel 3: Fuzzy matching de t√≠tulos (O(n))
    Para cada publicaci√≥n √∫nica U:
        similitud = SequenceMatcher(P.title, U.title).ratio()
        
        Si similitud >= umbral (default 0.95):
            Marcar como duplicado
            Continuar
    
    # Si no es duplicado:
    Agregar P a lista de √∫nicos
    Actualizar √≠ndice_doi
    Actualizar √≠ndice_hash
```

### Normalizaci√≥n de T√≠tulos

```python
def normalize_title(title: str) -> str:
    """
    1. Convertir a min√∫sculas
    2. Eliminar puntuaci√≥n
    3. Eliminar art√≠culos (a, an, the)
    4. Eliminar espacios extras
    """
    title = title.lower()
    title = re.sub(r'[^\w\s]', '', title)
    title = re.sub(r'^(a|an|the)\s+', '', title)
    title = ' '.join(title.split())
    return title
```

### Complejidad

- **Mejor caso**: O(n) cuando todos tienen DOI √∫nico
- **Caso promedio**: O(n log n)
- **Peor caso**: O(n¬≤) cuando se requiere fuzzy matching exhaustivo

---

## üìù Formato de Datos Unificado

### Modelo Publication

```python
{
  "id": "pub_abc123def456",
  "title": "Generative AI in Educational Contexts",
  "abstract": "This study explores...",
  "authors": [
    {
      "name": "Dr. Emily Johnson",
      "affiliation": "Stanford University",
      "country": "United States",
      "orcid": "0000-0002-1825-0097"
    }
  ],
  "keywords": ["generative ai", "education", "machine learning"],
  "doi": "10.1145/3544548.3580958",
  "publication_date": "2024-04-23",
  "publication_year": 2024,
  "journal": "ACM Transactions on Computer-Human Interaction",
  "volume": "31",
  "issue": "2",
  "pages": "1-42",
  "publisher": "ACM",
  "source": "crossref",
  "url": "https://doi.org/10.1145/3544548.3580958",
  "citation_count": 15,
  "publication_type": "article",
  "language": "en"
}
```

---

## üß™ Testing

### Ejecutar Pruebas

```bash
cd Backend
python test_requerimiento1.py
```

### Pruebas Incluidas

1. **Test de Deduplicaci√≥n**
   - Verifica detecci√≥n de duplicados exactos (DOI)
   - Verifica detecci√≥n por similitud de t√≠tulo
   - Verifica generaci√≥n de reporte

2. **Test de Descarga Completa**
   - Descarga real desde CrossRef
   - Unificaci√≥n de formatos
   - Exportaci√≥n a todos los formatos
   - Validaci√≥n de archivos generados

---

## üìà M√©tricas de Rendimiento

### Benchmarks (Hardware est√°ndar)

| Operaci√≥n | Tiempo | Throughput |
|-----------|--------|------------|
| Descarga 50 pubs (CrossRef) | ~30s | 1.67 pubs/s |
| Descarga 100 pubs | ~60s | 1.67 pubs/s |
| Deduplicaci√≥n 100 pubs | ~0.5s | 200 pubs/s |
| Exportaci√≥n JSON | ~0.1s | 1000 pubs/s |
| Exportaci√≥n BibTeX | ~0.2s | 500 pubs/s |

**Rate Limits:**
- CrossRef API: 50 req/s (p√∫blico), usando 1 req/s para ser cort√©s

---

## üêõ Troubleshooting

### Error: "Import could not be resolved"

**Causa:** Dependencias no instaladas

**Soluci√≥n:**
```bash
pip install -r requirements.txt
```

### Error: "aiohttp module not found"

**Soluci√≥n:**
```bash
pip install aiohttp
```

### Error: "Connection timeout"

**Causa:** Rate limit muy alto o problemas de red

**Soluci√≥n:** Aumentar el par√°metro `rate_limit` en el downloader:
```python
downloader = UnifiedDownloader(rate_limit=2.0)  # 2 segundos entre peticiones
```

### Duplicados no detectados correctamente

**Soluci√≥n:** Ajustar el umbral de similitud:
```python
downloader = UnifiedDownloader(similarity_threshold=0.90)  # M√°s permisivo
```

---

## üìö Documentaci√≥n API

Documentaci√≥n interactiva disponible en:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints Disponibles

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| POST | `/api/v1/data/download` | Iniciar descarga |
| GET | `/api/v1/data/status/{job_id}` | Estado del job |
| GET | `/api/v1/data/jobs` | Listar todos los jobs |
| GET | `/api/v1/data/unified` | Datos unificados |
| GET | `/api/v1/data/duplicates/{job_id}` | Reporte de duplicados |
| GET | `/api/v1/data/download/{job_id}/{format}` | Descargar resultados |
| GET | `/api/v1/data/sources` | Fuentes disponibles |

---

## ‚úÖ Checklist de Implementaci√≥n

- [x] Modelo de datos unificado (`Publication`)
- [x] Clase base abstracta para scrapers
- [x] Scraper de CrossRef implementado y funcional
- [x] Sistema de deduplicaci√≥n completo
- [x] Generaci√≥n de reporte de duplicados
- [x] Exportaci√≥n a JSON
- [x] Exportaci√≥n a BibTeX
- [x] Exportaci√≥n a RIS
- [x] Exportaci√≥n a CSV
- [x] Orquestador de descargas (`UnifiedDownloader`)
- [x] API REST con FastAPI
- [x] Script de pruebas automatizado
- [x] Documentaci√≥n completa
- [ ] Scraper de ACM (pendiente - requiere credenciales)
- [ ] Scraper de SAGE (pendiente - requiere credenciales)
- [ ] Scraper de ScienceDirect (pendiente - requiere credenciales)
- [ ] Tests unitarios con pytest
- [ ] Integraci√≥n con base de datos PostgreSQL
- [ ] Sistema de cach√© con Redis

---

## üë• Autores

- **Santiago Ovalle Cort√©s**
- **Juan Sebasti√°n Nore√±a**

**Supervisor Acad√©mico:** Carlos Andres Flores Villaraga  
**Curso:** An√°lisis de Algoritmos (2025-2)  
**Universidad del Quind√≠o**

---

## üìÑ Licencia

Proyecto acad√©mico - Universidad del Quind√≠o ¬© 2025
