# üìã PLAN DE IMPLEMENTACI√ìN DETALLADO
## Proyecto de An√°lisis Bibliom√©trico - Universidad del Quind√≠o

**Autores:** Santiago Ovalle Cort√©s, Juan Sebasti√°n Nore√±a  
**Curso:** An√°lisis de Algoritmos (2025-2)  
**Fecha de inicio:** 20 de Octubre, 2025  
**Dominio:** Inteligencia Artificial Generativa en Educaci√≥n  
**Cadena de b√∫squeda:** "generative artificial intelligence"

---

## üéØ OBJETIVO GENERAL

Implementar una plataforma web automatizada para an√°lisis bibliom√©trico avanzado de publicaciones cient√≠ficas sobre IA Generativa, integrando algoritmos cl√°sicos y modernos de ML/NLP, con capacidades de visualizaci√≥n interactiva y exportaci√≥n de resultados.

---

## üìä ESTADO ACTUAL DEL PROYECTO

### ‚úÖ Infraestructura Completada
- [x] Estructura de directorios Backend y Frontend
- [x] FastAPI configurado con CORS y middlewares
- [x] Dependencias Python instaladas (requirements.txt)
- [x] Aplicaci√≥n React con TypeScript y Vite
- [x] Dependencias frontend (MUI, D3, Plotly, Recharts)
- [x] Sistema de logging y manejo de errores

### ‚è≥ Pendiente de Implementaci√≥n
- [x] Requerimiento 1: Automatizaci√≥n de descarga de datos
- [ ] Requerimiento 2: Algoritmos de similitud textual
- [ ] Requerimiento 3: An√°lisis de frecuencias de conceptos
- [ ] Requerimiento 4: Clustering jer√°rquico y dendrogramas
- [ ] Requerimiento 5: Visualizaciones interactivas
- [ ] Requerimiento 6: Despliegue y documentaci√≥n t√©cnica

---

## üóìÔ∏è CRONOGRAMA DE IMPLEMENTACI√ìN

### **Fase 1: Requerimiento 1 - Automatizaci√≥n de Descarga** (5 d√≠as)
- D√≠as 1-2: Scrapers y conectores API
- D√≠a 3: Unificaci√≥n y eliminaci√≥n de duplicados
- D√≠as 4-5: Testing y validaci√≥n

### **Fase 2: Requerimiento 2 - Similitud Textual** (7 d√≠as)
- D√≠as 6-7: Algoritmos cl√°sicos (Levenshtein, TF-IDF, Jaccard, N-gramas)
- D√≠as 8-9: Algoritmos con IA (BERT, Sentence-BERT)
- D√≠as 10-11: Documentaci√≥n matem√°tica detallada
- D√≠a 12: UI para comparaci√≥n de abstracts

### **Fase 3: Requerimiento 3 - Frecuencias de Conceptos** (4 d√≠as)
- D√≠a 13: An√°lisis de frecuencias de categor√≠as predefinidas
- D√≠a 14: Generaci√≥n autom√°tica de palabras asociadas con NLP
- D√≠a 15: M√©tricas de precisi√≥n
- D√≠a 16: Visualizaciones de frecuencias

### **Fase 4: Requerimiento 4 - Clustering Jer√°rquico** (5 d√≠as)
- D√≠as 17-18: Preprocesamiento de texto y vectorizaci√≥n
- D√≠a 19: Implementaci√≥n de 3 algoritmos de clustering
- D√≠a 20: Generaci√≥n de dendrogramas interactivos con D3.js
- D√≠a 21: Evaluaci√≥n comparativa (Silhouette Score)

### **Fase 5: Requerimiento 5 - Visualizaciones** (4 d√≠as)
- D√≠a 22: Mapa de calor geogr√°fico (Plotly/Leaflet)
- D√≠a 23: Nube de palabras din√°mica (D3.js)
- D√≠a 24: L√≠nea temporal por a√±o y revista (Recharts)
- D√≠a 25: Exportaci√≥n a PDF (jsPDF + html2canvas)

### **Fase 6: Requerimiento 6 - Despliegue y Documentaci√≥n** (4 d√≠as)
- D√≠a 26: Documentaci√≥n t√©cnica de arquitectura
- D√≠a 27: Documentaci√≥n de algoritmos implementados
- D√≠a 28: Dockerizaci√≥n y CI/CD
- D√≠a 29: Despliegue en producci√≥n

### **Fase 7: Testing Final y Optimizaci√≥n** (2 d√≠as)
- D√≠a 30: Testing integral y correcciones
- D√≠a 31: Optimizaci√≥n de rendimiento

**TOTAL ESTIMADO: 31 d√≠as (‚âà 6-7 semanas)**

---

## üèóÔ∏è ARQUITECTURA DEL SISTEMA

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FRONTEND (React + TS)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   D3.js     ‚îÇ  ‚îÇ  Plotly.js  ‚îÇ  ‚îÇ  Recharts   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ Dendrogramas‚îÇ  ‚îÇ Mapas Calor ‚îÇ  ‚îÇ  L√≠neas     ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP/REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   BACKEND (FastAPI)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ            API ENDPOINTS (v1)                        ‚îÇ   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ  ‚îÇ /api/v1/data          - Data Acquisition Service    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ /api/v1/ml            - ML & NLP Service             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ /api/v1/analytics     - Analytics Service            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ /api/v1/viz           - Visualization Service        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ         SERVICIOS DE NEGOCIO                        ‚îÇ    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ
‚îÇ  ‚îÇ üîç DataAcquisitionService                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Scrapers (ACM, SAGE, ScienceDirect)           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Unificaci√≥n y deduplicaci√≥n                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ü§ñ MLAnalysisService                                ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Similitud cl√°sica (Levenshtein, TF-IDF, etc.) ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Similitud IA (BERT, Sentence-BERT)            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Clustering jer√°rquico (Ward, Average, etc.)   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ üìä AnalyticsService                                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Frecuencias de conceptos                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - M√©tricas bibliom√©tricas                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Generaci√≥n de palabras asociadas               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ üìà VisualizationService                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Mapas de calor geogr√°ficos                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Nubes de palabras                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - L√≠neas temporales                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    - Exportaci√≥n PDF                                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               CAPA DE DATOS                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ    Redis     ‚îÇ  ‚îÇ  File System ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Metadatos   ‚îÇ  ‚îÇ    Cache     ‚îÇ  ‚îÇ  Archivos    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Publicac.   ‚îÇ  ‚îÇ  Embeddings  ‚îÇ  ‚îÇ  CSV/BibTeX  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù REQUERIMIENTO 1: AUTOMATIZACI√ìN DE DESCARGA DE DATOS

### **Objetivo**
Automatizar la descarga de publicaciones cient√≠ficas desde ACM, SAGE y ScienceDirect, unificar los datos en un archivo √∫nico sin duplicados, y generar un reporte de productos eliminados.

### **Componentes a Implementar**

#### 1.1. **Scrapers y Conectores API**

**Archivos a crear:**
```
Backend/app/services/data_acquisition/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ base_scraper.py          # Clase base abstracta
‚îú‚îÄ‚îÄ acm_scraper.py           # Scraper para ACM Digital Library
‚îú‚îÄ‚îÄ sage_scraper.py          # Scraper para SAGE Publications
‚îú‚îÄ‚îÄ sciencedirect_scraper.py # Scraper para ScienceDirect
‚îú‚îÄ‚îÄ unified_downloader.py    # Orquestador de descargas
‚îî‚îÄ‚îÄ parsers/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ bibtex_parser.py     # Parser para formato BibTex
    ‚îú‚îÄ‚îÄ ris_parser.py        # Parser para formato RIS
    ‚îî‚îÄ‚îÄ csv_parser.py        # Parser para formato CSV
```

**Funcionalidades clave:**

```python
# base_scraper.py - Clase base abstracta
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseScraper(ABC):
    """Clase base para todos los scrapers de bases de datos cient√≠ficas."""
    
    @abstractmethod
    async def search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Busca publicaciones con la query especificada."""
        pass
    
    @abstractmethod
    async def download_metadata(self, publication_id: str) -> Dict[str, Any]:
        """Descarga metadatos completos de una publicaci√≥n."""
        pass
    
    @abstractmethod
    def export_to_format(self, publications: List[Dict], format: str) -> str:
        """Exporta publicaciones al formato especificado (bibtex, ris, csv)."""
        pass
```

**Tecnolog√≠as:**
- `scrapy` para scraping web
- `aiohttp` para peticiones as√≠ncronas
- `crossref-commons` para API de CrossRef
- `scholarly` para b√∫squedas acad√©micas
- `habanero` para API de CrossRef
- `beautifulsoup4` para parsing HTML

#### 1.2. **Sistema de Unificaci√≥n y Deduplicaci√≥n**

**Archivos a crear:**
```
Backend/app/services/data_acquisition/
‚îú‚îÄ‚îÄ deduplicator.py          # Sistema de eliminaci√≥n de duplicados
‚îú‚îÄ‚îÄ unifier.py               # Unificaci√≥n de formatos
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ publication.py       # Modelo de datos unificado
    ‚îî‚îÄ‚îÄ duplicate_report.py  # Modelo de reporte de duplicados
```

**Algoritmo de deduplicaci√≥n:**

```python
# deduplicator.py
import hashlib
from typing import List, Tuple, Dict
from difflib import SequenceMatcher

class Deduplicator:
    """Sistema inteligente de eliminaci√≥n de duplicados."""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.similarity_threshold = similarity_threshold
        self.duplicates_report = []
    
    def calculate_title_similarity(self, title1: str, title2: str) -> float:
        """Calcula similitud entre t√≠tulos usando SequenceMatcher."""
        return SequenceMatcher(None, title1.lower(), title2.lower()).ratio()
    
    def generate_hash(self, publication: Dict) -> str:
        """Genera hash √∫nico basado en t√≠tulo y DOI."""
        identifier = f"{publication.get('title', '')}_{publication.get('doi', '')}"
        return hashlib.md5(identifier.encode()).hexdigest()
    
    def deduplicate(
        self, 
        publications: List[Dict]
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        Elimina duplicados y retorna:
        - Lista de publicaciones √∫nicas
        - Lista de duplicados eliminados
        """
        unique_publications = []
        duplicates = []
        seen_hashes = set()
        
        for pub in publications:
            pub_hash = self.generate_hash(pub)
            
            # Verificar duplicados exactos por hash
            if pub_hash in seen_hashes:
                duplicates.append(pub)
                continue
            
            # Verificar duplicados por similitud de t√≠tulo
            is_duplicate = False
            for unique_pub in unique_publications:
                similarity = self.calculate_title_similarity(
                    pub.get('title', ''),
                    unique_pub.get('title', '')
                )
                if similarity >= self.similarity_threshold:
                    duplicates.append(pub)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_publications.append(pub)
                seen_hashes.add(pub_hash)
        
        return unique_publications, duplicates
```

#### 1.3. **Endpoints API**

**Archivos a crear:**
```
Backend/app/api/v1/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ data_acquisition.py  # Endpoints para descarga y unificaci√≥n
```

**Endpoints a implementar:**

```python
# data_acquisition.py
from fastapi import APIRouter, BackgroundTasks, HTTPException
from typing import List, Optional
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/data", tags=["Data Acquisition"])

class DownloadRequest(BaseModel):
    query: str
    sources: List[str]  # ['acm', 'sage', 'sciencedirect']
    max_results_per_source: int = 100
    export_format: str = 'json'  # json, bibtex, ris, csv

class DownloadResponse(BaseModel):
    job_id: str
    status: str
    message: str

@router.post("/download", response_model=DownloadResponse)
async def start_download(
    request: DownloadRequest,
    background_tasks: BackgroundTasks
):
    """
    Inicia descarga autom√°tica desde m√∫ltiples fuentes.
    
    - **query**: Cadena de b√∫squeda (ej: "generative artificial intelligence")
    - **sources**: Lista de bases de datos a consultar
    - **max_results_per_source**: Cantidad m√°xima de resultados por fuente
    - **export_format**: Formato de exportaci√≥n deseado
    """
    # Implementaci√≥n con Celery para tareas en background
    pass

@router.get("/status/{job_id}")
async def get_download_status(job_id: str):
    """Consulta el estado de una descarga en proceso."""
    pass

@router.get("/unified")
async def get_unified_data(
    format: Optional[str] = 'json',
    include_metadata: bool = True
):
    """Obtiene los datos unificados sin duplicados."""
    pass

@router.get("/duplicates")
async def get_duplicates_report():
    """Obtiene el reporte de publicaciones duplicadas eliminadas."""
    pass
```

#### 1.4. **Modelos de Datos**

```python
# Backend/app/models/publication.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import date

class Author(BaseModel):
    name: str
    affiliation: Optional[str] = None
    email: Optional[str] = None
    country: Optional[str] = None

class Publication(BaseModel):
    """Modelo unificado de publicaci√≥n cient√≠fica."""
    
    id: str = Field(description="ID √∫nico generado")
    title: str = Field(description="T√≠tulo del art√≠culo")
    abstract: str = Field(description="Resumen del art√≠culo")
    authors: List[Author] = Field(description="Lista de autores")
    keywords: List[str] = Field(default=[], description="Palabras clave")
    doi: Optional[str] = Field(None, description="DOI del art√≠culo")
    publication_date: Optional[date] = Field(None, description="Fecha de publicaci√≥n")
    journal: Optional[str] = Field(None, description="Revista o conferencia")
    source: str = Field(description="Fuente de datos (acm, sage, sciencedirect)")
    url: Optional[str] = Field(None, description="URL del art√≠culo")
    citation_count: Optional[int] = Field(0, description="N√∫mero de citas")
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "pub_001",
                "title": "Generative AI in Educational Contexts",
                "abstract": "This study explores...",
                "authors": [
                    {
                        "name": "John Doe",
                        "affiliation": "MIT",
                        "country": "USA"
                    }
                ],
                "keywords": ["Generative AI", "Education", "Machine Learning"],
                "doi": "10.1145/example",
                "publication_date": "2024-06-15",
                "journal": "ACM Transactions on Computer Education",
                "source": "acm"
            }
        }
```

#### 1.5. **Testing y Validaci√≥n**

**Archivos a crear:**
```
Backend/tests/
‚îú‚îÄ‚îÄ test_data_acquisition/
‚îÇ   ‚îú‚îÄ‚îÄ test_scrapers.py
‚îÇ   ‚îú‚îÄ‚îÄ test_deduplicator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_parsers.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api_endpoints.py
```

**Tests a implementar:**

```python
# test_deduplicator.py
import pytest
from app.services.data_acquisition.deduplicator import Deduplicator

def test_exact_duplicate_detection():
    """Verifica detecci√≥n de duplicados exactos."""
    publications = [
        {"title": "AI in Education", "doi": "10.1145/123"},
        {"title": "AI in Education", "doi": "10.1145/123"},  # Duplicado
    ]
    
    dedup = Deduplicator()
    unique, duplicates = dedup.deduplicate(publications)
    
    assert len(unique) == 1
    assert len(duplicates) == 1

def test_similar_title_detection():
    """Verifica detecci√≥n de duplicados por similitud."""
    publications = [
        {"title": "Generative AI in Education", "doi": "10.1145/123"},
        {"title": "Generative AI in Educational Contexts", "doi": "10.1145/456"},
    ]
    
    dedup = Deduplicator(similarity_threshold=0.8)
    unique, duplicates = dedup.deduplicate(publications)
    
    # Deber√≠a detectar como duplicados si similitud > 0.8
    assert len(unique) + len(duplicates) == 2
```

---

## üìù REQUERIMIENTO 2: ALGORITMOS DE SIMILITUD TEXTUAL

### **Objetivo**
Implementar 6 algoritmos de similitud textual (4 cl√°sicos + 2 con IA) con documentaci√≥n matem√°tica detallada y UI para comparaci√≥n de abstracts.

### **Componentes a Implementar**

#### 2.1. **Algoritmos Cl√°sicos de Similitud**

**Archivos a crear:**
```
Backend/app/services/ml_analysis/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ similarity/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_similarity.py        # Clase base abstracta
‚îÇ   ‚îú‚îÄ‚îÄ levenshtein.py            # Distancia de edici√≥n
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_cosine.py           # TF-IDF + Similitud del Coseno
‚îÇ   ‚îú‚îÄ‚îÄ jaccard.py                # Coeficiente de Jaccard
‚îÇ   ‚îú‚îÄ‚îÄ ngrams.py                 # Similitud por N-gramas
‚îÇ   ‚îú‚îÄ‚îÄ bert_embeddings.py        # BERT Embeddings
‚îÇ   ‚îú‚îÄ‚îÄ sentence_bert.py          # Sentence-BERT
‚îÇ   ‚îî‚îÄ‚îÄ similarity_analyzer.py    # Orquestador de an√°lisis
```

**2.1.1. Distancia de Levenshtein**

```python
# levenshtein.py
import numpy as np
from typing import Tuple, List, Dict

class LevenshteinSimilarity:
    """
    Implementaci√≥n de la Distancia de Levenshtein (Edit Distance).
    
    FUNDAMENTO MATEM√ÅTICO:
    =====================
    La distancia de Levenshtein entre dos cadenas s1 y s2 es el n√∫mero m√≠nimo
    de operaciones de edici√≥n (inserci√≥n, eliminaci√≥n, sustituci√≥n) necesarias
    para transformar s1 en s2.
    
    ALGORITMO DE PROGRAMACI√ìN DIN√ÅMICA:
    ===================================
    Sea DP[i][j] = distancia entre s1[0...i-1] y s2[0...j-1]
    
    Casos base:
    - DP[0][j] = j (insertar j caracteres)
    - DP[i][0] = i (eliminar i caracteres)
    
    Recurrencia:
    DP[i][j] = {
        DP[i-1][j-1]                           si s1[i-1] == s2[j-1]
        1 + min(DP[i-1][j],                    eliminaci√≥n
                DP[i][j-1],                    inserci√≥n
                DP[i-1][j-1])                  sustituci√≥n
                                               en caso contrario
    }
    
    COMPLEJIDAD:
    - Tiempo: O(m * n) donde m = len(s1), n = len(s2)
    - Espacio: O(m * n) para la matriz DP
    
    SIMILITUD NORMALIZADA:
    similarity = 1 - (distance / max(len(s1), len(s2)))
    """
    
    def __init__(self):
        self.name = "Distancia de Levenshtein"
        self.description = "Medida de similitud basada en operaciones de edici√≥n"
    
    def calculate_distance(
        self, 
        text1: str, 
        text2: str,
        return_matrix: bool = False
    ) -> Tuple[int, np.ndarray]:
        """
        Calcula la distancia de Levenshtein usando programaci√≥n din√°mica.
        
        Args:
            text1: Primer texto
            text2: Segundo texto
            return_matrix: Si True, retorna la matriz DP completa
        
        Returns:
            Tupla (distancia, matriz_dp)
        """
        m, n = len(text1), len(text2)
        
        # Inicializar matriz DP
        dp = np.zeros((m + 1, n + 1), dtype=int)
        
        # Casos base
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        # Llenar matriz usando programaci√≥n din√°mica
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if text1[i-1] == text2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(
                        dp[i-1][j],      # Eliminaci√≥n
                        dp[i][j-1],      # Inserci√≥n
                        dp[i-1][j-1]     # Sustituci√≥n
                    )
        
        distance = dp[m][n]
        return (distance, dp) if return_matrix else (distance, None)
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud normalizada [0, 1].
        
        similarity = 1 - (distance / max_length)
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (id√©nticos)
        """
        distance, _ = self.calculate_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1.0 - (distance / max_len)
        return similarity
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        An√°lisis detallado paso a paso con explicaci√≥n matem√°tica.
        
        Returns:
            Diccionario con:
            - distance: Distancia de Levenshtein
            - similarity: Similitud normalizada
            - dp_matrix: Matriz de programaci√≥n din√°mica
            - operations: Lista de operaciones necesarias
            - explanation: Explicaci√≥n paso a paso
        """
        distance, dp_matrix = self.calculate_distance(text1, text2, return_matrix=True)
        similarity = self.calculate_similarity(text1, text2)
        
        # Reconstruir secuencia de operaciones
        operations = self._reconstruct_operations(text1, text2, dp_matrix)
        
        # Generar explicaci√≥n detallada
        explanation = self._generate_explanation(
            text1, text2, distance, similarity, operations
        )
        
        return {
            "algorithm": self.name,
            "distance": distance,
            "similarity": similarity,
            "dp_matrix": dp_matrix.tolist(),
            "operations": operations,
            "explanation": explanation,
            "complexity": {
                "time": f"O({len(text1)} √ó {len(text2)}) = O({len(text1) * len(text2)})",
                "space": f"O({len(text1)} √ó {len(text2)}) = O({len(text1) * len(text2)})"
            }
        }
    
    def _reconstruct_operations(
        self, 
        text1: str, 
        text2: str, 
        dp: np.ndarray
    ) -> List[Dict[str, str]]:
        """Reconstruye la secuencia de operaciones de edici√≥n."""
        operations = []
        i, j = len(text1), len(text2)
        
        while i > 0 or j > 0:
            if i == 0:
                operations.append({
                    "type": "insert",
                    "char": text2[j-1],
                    "position": j-1
                })
                j -= 1
            elif j == 0:
                operations.append({
                    "type": "delete",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
            elif text1[i-1] == text2[j-1]:
                operations.append({
                    "type": "match",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
                j -= 1
            else:
                # Determinar operaci√≥n √≥ptima
                delete_cost = dp[i-1][j]
                insert_cost = dp[i][j-1]
                substitute_cost = dp[i-1][j-1]
                
                min_cost = min(delete_cost, insert_cost, substitute_cost)
                
                if min_cost == substitute_cost:
                    operations.append({
                        "type": "substitute",
                        "from_char": text1[i-1],
                        "to_char": text2[j-1],
                        "position": i-1
                    })
                    i -= 1
                    j -= 1
                elif min_cost == delete_cost:
                    operations.append({
                        "type": "delete",
                        "char": text1[i-1],
                        "position": i-1
                    })
                    i -= 1
                else:
                    operations.append({
                        "type": "insert",
                        "char": text2[j-1],
                        "position": j-1
                    })
                    j -= 1
        
        return list(reversed(operations))
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        distance: int,
        similarity: float,
        operations: List[Dict]
    ) -> str:
        """Genera explicaci√≥n matem√°tica detallada."""
        explanation = f"""
        AN√ÅLISIS DE SIMILITUD CON DISTANCIA DE LEVENSHTEIN
        ==================================================
        
        Textos analizados:
        - Texto 1 (m={len(text1)}): "{text1[:50]}..."
        - Texto 2 (n={len(text2)}): "{text2[:50]}..."
        
        RESULTADOS:
        -----------
        - Distancia de Levenshtein: {distance}
        - Similitud normalizada: {similarity:.4f} ({similarity*100:.2f}%)
        
        OPERACIONES NECESARIAS ({len([op for op in operations if op['type'] != 'match'])} operaciones):
        ---------------------
        """
        
        for i, op in enumerate(operations[:10], 1):  # Primeras 10 operaciones
            if op['type'] == 'substitute':
                explanation += f"{i}. Sustituir '{op['from_char']}' por '{op['to_char']}' en posici√≥n {op['position']}\n"
            elif op['type'] == 'insert':
                explanation += f"{i}. Insertar '{op['char']}' en posici√≥n {op['position']}\n"
            elif op['type'] == 'delete':
                explanation += f"{i}. Eliminar '{op['char']}' de posici√≥n {op['position']}\n"
        
        if len(operations) > 10:
            explanation += f"... ({len(operations) - 10} operaciones m√°s)\n"
        
        return explanation
```

**2.1.2. TF-IDF + Similitud del Coseno**

```python
# tfidf_cosine.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict, List, Tuple

class TFIDFCosineSimilarity:
    """
    Implementaci√≥n de Similitud TF-IDF + Coseno.
    
    FUNDAMENTO MATEM√ÅTICO:
    =====================
    
    1. TF-IDF (Term Frequency - Inverse Document Frequency):
       
       TF(t,d) = Frecuencia del t√©rmino t en el documento d
                 -----------------------------------------------
                 N√∫mero total de t√©rminos en el documento d
       
       IDF(t,D) = log( N√∫mero total de documentos en D )
                      ------------------------------------
                      N√∫mero de documentos que contienen t
       
       TF-IDF(t,d,D) = TF(t,d) √ó IDF(t,D)
    
    2. Similitud del Coseno:
       
       cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
       
       Donde:
       - A ¬∑ B = Producto punto de vectores A y B = Œ£(Ai √ó Bi)
       - ||A|| = Norma euclidiana de A = ‚àö(Œ£(Ai¬≤))
       - ||B|| = Norma euclidiana de B = ‚àö(Œ£(Bi¬≤))
       
       Rango: [-1, 1]
       - 1 = Vectores id√©nticos en direcci√≥n
       - 0 = Vectores ortogonales
       - -1 = Vectores opuestos
    
    INTERPRETACI√ìN:
    - La similitud del coseno mide el √°ngulo entre dos vectores en el espacio
      de caracter√≠sticas TF-IDF.
    - No considera la magnitud, solo la orientaci√≥n.
    - Ideal para comparar documentos de diferentes longitudes.
    
    COMPLEJIDAD:
    - Vectorizaci√≥n TF-IDF: O(n √ó m) donde n = docs, m = vocabulario
    - Similitud del coseno: O(m) por par de documentos
    """
    
    def __init__(
        self, 
        max_features: int = 5000,
        ngram_range: Tuple[int, int] = (1, 2),
        min_df: int = 1,
        max_df: float = 0.9
    ):
        """
        Inicializa el analizador TF-IDF + Coseno.
        
        Args:
            max_features: N√∫mero m√°ximo de caracter√≠sticas (t√©rminos)
            ngram_range: Rango de n-gramas a considerar
            min_df: Frecuencia m√≠nima de documento
            max_df: Frecuencia m√°xima de documento (fracci√≥n)
        """
        self.name = "TF-IDF + Similitud del Coseno"
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=min_df,
            max_df=max_df,
            stop_words='english',
            lowercase=True,
            strip_accents='unicode'
        )
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud del coseno entre dos textos.
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (id√©nticos)
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return float(similarity)
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        An√°lisis detallado con vectores TF-IDF y explicaci√≥n matem√°tica.
        
        Returns:
            Diccionario con an√°lisis completo paso a paso
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Obtener nombres de caracter√≠sticas
        feature_names = self.vectorizer.get_feature_names_out()
        
        # Extraer vectores TF-IDF
        vector1 = tfidf_matrix[0].toarray()[0]
        vector2 = tfidf_matrix[1].toarray()[0]
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        # Identificar t√©rminos m√°s relevantes
        top_terms_text1 = self._get_top_terms(vector1, feature_names, top_n=10)
        top_terms_text2 = self._get_top_terms(vector2, feature_names, top_n=10)
        
        # Calcular normas euclidianas
        norm1 = np.linalg.norm(vector1)
        norm2 = np.linalg.norm(vector2)
        
        # Calcular producto punto
        dot_product = np.dot(vector1, vector2)
        
        # Generar explicaci√≥n detallada
        explanation = self._generate_explanation(
            text1, text2, vector1, vector2, 
            dot_product, norm1, norm2, similarity,
            top_terms_text1, top_terms_text2
        )
        
        return {
            "algorithm": self.name,
            "similarity": float(similarity),
            "vectors": {
                "text1": {
                    "vector": vector1.tolist(),
                    "norm": float(norm1),
                    "top_terms": top_terms_text1
                },
                "text2": {
                    "vector": vector2.tolist(),
                    "norm": float(norm2),
                    "top_terms": top_terms_text2
                }
            },
            "dot_product": float(dot_product),
            "cosine_angle_degrees": float(np.degrees(np.arccos(similarity))),
            "vocabulary_size": len(feature_names),
            "explanation": explanation,
            "complexity": {
                "vectorization": f"O(n √ó m) donde n=2 documentos, m={len(feature_names)} t√©rminos",
                "similarity": "O(m) para producto punto y normas"
            }
        }
    
    def _get_top_terms(
        self, 
        vector: np.ndarray, 
        feature_names: np.ndarray, 
        top_n: int = 10
    ) -> List[Dict[str, any]]:
        """Extrae los t√©rminos con mayor peso TF-IDF."""
        # Obtener √≠ndices de t√©rminos ordenados por peso TF-IDF
        top_indices = np.argsort(vector)[-top_n:][::-1]
        
        top_terms = []
        for idx in top_indices:
            if vector[idx] > 0:
                top_terms.append({
                    "term": feature_names[idx],
                    "tfidf_weight": float(vector[idx])
                })
        
        return top_terms
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        vector1: np.ndarray,
        vector2: np.ndarray,
        dot_product: float,
        norm1: float,
        norm2: float,
        similarity: float,
        top_terms1: List[Dict],
        top_terms2: List[Dict]
    ) -> str:
        """Genera explicaci√≥n matem√°tica detallada."""
        angle_degrees = np.degrees(np.arccos(np.clip(similarity, -1, 1)))
        
        explanation = f"""
        AN√ÅLISIS DE SIMILITUD CON TF-IDF + COSENO
        =========================================
        
        Textos analizados:
        - Texto 1: "{text1[:50]}..."
        - Texto 2: "{text2[:50]}..."
        
        PASO 1: VECTORIZACI√ìN TF-IDF
        -----------------------------
        Vocabulario total: {len(vector1)} t√©rminos √∫nicos
        
        T√©rminos m√°s relevantes en Texto 1:
        """
        
        for i, term_info in enumerate(top_terms1[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += "\nT√©rminos m√°s relevantes en Texto 2:\n"
        for i, term_info in enumerate(top_terms2[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += f"""
        
        PASO 2: C√ÅLCULO DE SIMILITUD DEL COSENO
        ----------------------------------------
        F√≥rmula: cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
        
        Producto punto (A ¬∑ B): {dot_product:.6f}
        Norma de A (||A||): {norm1:.6f}
        Norma de B (||B||): {norm2:.6f}
        
        Similitud del coseno: {dot_product:.6f} / ({norm1:.6f} √ó {norm2:.6f})
                            = {dot_product:.6f} / {norm1 * norm2:.6f}
                            = {similarity:.6f}
        
        √Ångulo entre vectores: {angle_degrees:.2f}¬∞
        
        INTERPRETACI√ìN:
        ---------------
        - Similitud: {similarity:.4f} ({similarity*100:.2f}%)
        - √Ångulo: {angle_degrees:.2f}¬∞ (0¬∞ = id√©nticos, 90¬∞ = ortogonales)
        - Los documentos son {"muy similares" if similarity > 0.8 else "moderadamente similares" if similarity > 0.5 else "poco similares"}
        """
        
        return explanation
```

**(Continuar√© con los otros 4 algoritmos de similitud en el siguiente mensaje debido a la longitud...)**

Ahora voy a crear el resto de los algoritmos de similitud y los siguientes requerimientos de manera estructurada.

---

## üì¶ PR√ìXIMOS PASOS A IMPLEMENTAR

Ahora voy a crear la estructura completa del proyecto con todos los archivos necesarios. Comenzar√© implementando los componentes paso a paso.

¬øDeseas que contin√∫e con:

1. **Implementaci√≥n completa del Requerimiento 1** (Scrapers + Deduplicaci√≥n)
2. **Completar todos los algoritmos de similitud** (6 algoritmos completos)
3. **Implementaci√≥n del Requerimiento 3** (Frecuencias de conceptos)
4. **Implementaci√≥n del Requerimiento 4** (Clustering y dendrogramas)
5. **O prefieres que cree primero la estructura completa** y luego implementemos cada m√≥dulo?

Tambi√©n puedo:
- Crear scripts de inicializaci√≥n de base de datos
- Configurar Docker y Docker Compose
- Implementar el frontend con los componentes de visualizaci√≥n
- Crear tests automatizados

**¬øPor d√≥nde quieres que empiece?** Te recomiendo seguir el orden de los requerimientos para tener un flujo de trabajo l√≥gico.
