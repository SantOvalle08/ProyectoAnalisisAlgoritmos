# ğŸ“‹ PLAN DE IMPLEMENTACIÃ“N - PROYECTO BIBLIOMÃ‰TRICO# ğŸ“‹ PLAN DE IMPLEMENTACIÃ“N - ACTUALIZADO# ğŸ“‹ PLAN DE IMPLEMENTACIÃ“N DETALLADO



**Universidad del QuindÃ­o - AnÃ¡lisis de Algoritmos (2025-2)**  ## Proyecto de AnÃ¡lisis BibliomÃ©trico - Universidad del QuindÃ­o## Proyecto de AnÃ¡lisis BibliomÃ©trico - Universidad del QuindÃ­o

**Autores:** Santiago Ovalle CortÃ©s, Juan SebastiÃ¡n NoreÃ±a  

**Ãšltima actualizaciÃ³n:** 23 de Enero, 2025  

**Dominio:** Inteligencia Artificial Generativa en EducaciÃ³n  

**Cadena de bÃºsqueda:** "generative artificial intelligence"**Autores:** Santiago Ovalle CortÃ©s, Juan SebastiÃ¡n NoreÃ±a  **Autores:** Santiago Ovalle CortÃ©s, Juan SebastiÃ¡n NoreÃ±a  



---**Curso:** AnÃ¡lisis de Algoritmos (2025-2)  **Curso:** AnÃ¡lisis de Algoritmos (2025-2)  



## ğŸ¯ OBJETIVO GENERAL**Ãšltima actualizaciÃ³n:** 23 de Enero, 2025  **Fecha de inicio:** 20 de Octubre, 2025  



Implementar una plataforma web automatizada para anÃ¡lisis bibliomÃ©trico de publicaciones cientÃ­ficas sobre IA Generativa, integrando algoritmos clÃ¡sicos y modernos de ML/NLP, con capacidades de visualizaciÃ³n interactiva y exportaciÃ³n de resultados.**Dominio:** Inteligencia Artificial Generativa en EducaciÃ³n  **Dominio:** Inteligencia Artificial Generativa en EducaciÃ³n  



---**Cadena de bÃºsqueda:** "generative artificial intelligence"**Cadena de bÃºsqueda:** "generative artificial intelligence"



## ğŸ“Š ESTADO ACTUAL DEL PROYECTO (23 de Enero, 2025)



### âœ… COMPLETADO (40%)------



#### **âœ… Requerimiento 2: Algoritmos de Similitud Textual** (100%)

- âœ… 6 algoritmos implementados: Levenshtein, TF-IDF + Coseno, Jaccard, N-gramas, BERT, Sentence-BERT

- âœ… 8 endpoints REST funcionales con documentaciÃ³n OpenAPI## ğŸ¯ OBJETIVO GENERAL## ğŸ¯ OBJETIVO GENERAL

- âœ… Suite de tests completa (26 tests pasando)

- âœ… DocumentaciÃ³n matemÃ¡tica paso a paso para cada algoritmo

- âœ… UI capaz de comparar abstracts de publicaciones

Implementar una plataforma web automatizada para anÃ¡lisis bibliomÃ©trico avanzado de publicaciones cientÃ­ficas sobre IA Generativa, integrando algoritmos clÃ¡sicos y modernos de ML/NLP, con capacidades de visualizaciÃ³n interactiva y exportaciÃ³n de resultados.Implementar una plataforma web automatizada para anÃ¡lisis bibliomÃ©trico avanzado de publicaciones cientÃ­ficas sobre IA Generativa, integrando algoritmos clÃ¡sicos y modernos de ML/NLP, con capacidades de visualizaciÃ³n interactiva y exportaciÃ³n de resultados.

**Archivos:**

- `app/services/ml_analysis/similarity/` - 7 mÃ³dulos (1,800 lÃ­neas)

- `app/api/v1/similarity.py` - 742 lÃ­neas

- `tests/test_similarity_*.py` - 26 tests------



#### **âœ… Parsers BibliogrÃ¡ficos** (100%)

- âœ… BibTeX Parser con manejo robusto de braces anidados

- âœ… RIS Parser con soporte completo de tags## ğŸ“Š ESTADO ACTUAL DEL PROYECTO## ğŸ“Š ESTADO ACTUAL DEL PROYECTO

- âœ… CSV Parser con normalizaciÃ³n flexible

- âœ… Unificador automÃ¡tico con detecciÃ³n de formatos

- âœ… Suite de tests completa (7 tests pasando)

### âœ… **Completado (33%)**### âœ… Infraestructura Completada

**Archivos:**

- `app/services/data_acquisition/parsers/` - 4 mÃ³dulos (1,380 lÃ­neas)- [x] Estructura de directorios Backend y Frontend

- `test_parsers.py` - 7 tests

#### **Requerimiento 2: Algoritmos de Similitud Textual** âœ…- [x] FastAPI configurado con CORS y middlewares

#### **âš™ï¸ Infraestructura Base** (100%)

- âœ… FastAPI con CORS y middlewares- [x] 6 algoritmos implementados (Levenshtein, TF-IDF, Jaccard, N-gramas, BERT, Sentence-BERT)- [x] Dependencias Python instaladas (requirements.txt)

- âœ… Estructura modular Backend + Frontend

- âœ… Sistema de logging y manejo de errores- [x] 8 endpoints REST funcionales- [x] AplicaciÃ³n React con TypeScript y Vite

- âœ… Dependencias instaladas (requirements.txt con 79 paquetes)

- âœ… .gitignore configurado- [x] Suite de tests completa (32/32 pasando)- [x] Dependencias frontend (MUI, D3, Plotly, Recharts)

- âœ… Tests automatizados (pytest)

### âœ… **Completado (45%)**

#### **Requerimiento 2: Algoritmos de Similitud Textual** âœ…

- [x] 6 algoritmos implementados (Levenshtein, TF-IDF, Jaccard, N-gramas, BERT, Sentence-BERT)
- [x] 8 endpoints REST funcionales
- [x] Suite de tests completa (39/39 pasando)
- [x] DocumentaciÃ³n matemÃ¡tica integrada
- [x] Sistema de logging y manejo de errores

- **Archivos:** 7 mÃ³dulos de similitud + 3 archivos de tests (2,500 lÃ­neas)

---

#### **Requerimiento 1: AutomatizaciÃ³n de Descarga** âœ…

**Implementado:**
- âœ… **ACM Digital Library Scraper** - Web scraping con BeautifulSoup (450 lÃ­neas)
- âœ… **SAGE Journals Scraper** - Multi-selector CSS (448 lÃ­neas)
- âœ… **ScienceDirect Scraper** - API de Elsevier con mock data (580 lÃ­neas)
- âœ… **CrossRefScraper** - API de CrossRef totalmente funcional
- âœ… **Sistema de deduplicaciÃ³n** - DOI, hash MD5, fuzzy matching
- âœ… **UnifiedDownloader** - Coordinador de mÃºltiples fuentes
- âœ… **Suite completa de tests** - 17/18 tests pasando (1 skipped)
- âœ… **8 Endpoints API de descarga** - `/api/v1/data/*`

**Archivos:**
- `app/services/data_acquisition/*.py` - 2,400 lÃ­neas
- `test_data_acquisition.py` - 450 lÃ­neas (18 tests)

---

#### **Requerimiento 6: Parsers BibliogrÃ¡ficos** âœ…

- [x] BibTeXParser con manejo robusto de braces anidados
- [x] RISParser con soporte completo de tags
- [x] CSVParser con normalizaciÃ³n flexible
- [x] PublicationUnifier con auto-detecciÃ³n de formatos
- [x] Suite de tests completa (7/7 pasando)

- **Archivos:** 4 parsers + 1 archivo de tests (1,380 lÃ­neas)

---

### â³ Pendiente de ImplementaciÃ³n (55%)

- [ ] Requerimiento 3: AnÃ¡lisis de frecuencias de conceptos
- [ ] Requerimiento 4: Clustering jerÃ¡rquico y dendrogramas
- [ ] Requerimiento 5: Visualizaciones interactivas
- [ ] Requerimiento 6: Despliegue y documentaciÃ³n tÃ©cnica

---

## ğŸ—“ï¸ CRONOGRAMA DE IMPLEMENTACIÃ“N

### **Fase 1: Requerimiento 1 - AutomatizaciÃ³n de Descarga** âœ… COMPLETADO

- âœ… DÃ­as 1-2: Scrapers (ACM, SAGE, ScienceDirect) y conectores API
- âœ… DÃ­a 3: UnificaciÃ³n y eliminaciÃ³n de duplicados
- âœ… DÃ­as 4-5: Testing y validaciÃ³n (56/57 tests pasando)

- âŒ CÃ¡lculo de precisiÃ³n de palabras generadas

- âŒ VisualizaciÃ³n de frecuencias- [ ] Endpoints REST



**CategorÃ­a a analizar:**- **Prioridad:** ALTA | **Estimado:** 6 horas### **Fase 2: Requerimiento 2 - Similitud Textual** (7 dÃ­as)

```

Concepts of Generative AI in Education:- DÃ­as 6-7: Algoritmos clÃ¡sicos (Levenshtein, TF-IDF, Jaccard, N-gramas)

- Generative models, Prompting, Machine learning, Multimodality

- Fine-tuning, Training data, Algorithmic bias#### **Requerimiento 4: Clustering JerÃ¡rquico**- DÃ­as 8-9: Algoritmos con IA (BERT, Sentence-BERT)

- Explainability, Transparency, Ethics, Privacy

- Personalization, Human-AI interaction, AI literacy, Co-creation- [ ] Algoritmo de clustering- DÃ­as 10-11: DocumentaciÃ³n matemÃ¡tica detallada

```

- [ ] GeneraciÃ³n de dendrogramas- DÃ­a 12: UI para comparaciÃ³n de abstracts

**Estimado:** 2 dÃ­as (12 horas)

- [ ] IdentificaciÃ³n automÃ¡tica de grupos

#### **Requerimiento 4: Clustering JerÃ¡rquico** (0%)

- âŒ Preprocesamiento de abstracts- [ ] MÃ©tricas de evaluaciÃ³n### **Fase 3: Requerimiento 3 - Frecuencias de Conceptos** (4 dÃ­as)

- âŒ ImplementaciÃ³n de 3 algoritmos de clustering (Ward, Average Linkage, Complete Linkage)

- âŒ GeneraciÃ³n de dendrogramas interactivos- [ ] Endpoints REST- DÃ­a 13: AnÃ¡lisis de frecuencias de categorÃ­as predefinidas

- âŒ EvaluaciÃ³n comparativa (Silhouette Score)

- **Prioridad:** ALTA | **Estimado:** 6 horas- DÃ­a 14: GeneraciÃ³n automÃ¡tica de palabras asociadas con NLP

**Estimado:** 3 dÃ­as (18 horas)

- DÃ­a 15: MÃ©tricas de precisiÃ³n

#### **Requerimiento 5: Visualizaciones Interactivas** (0%)

- âŒ Mapa de calor geogrÃ¡fico (distribuciÃ³n por paÃ­s del primer autor)#### **Requerimiento 5: Scrapers Adicionales**- DÃ­a 16: Visualizaciones de frecuencias

- âŒ Nube de palabras dinÃ¡mica (tÃ©rminos frecuentes en abstracts y keywords)

- âŒ LÃ­nea temporal de publicaciones por aÃ±o y revista- [ ] ACM Digital Library scraper

- âŒ ExportaciÃ³n a PDF de visualizaciones

- [ ] SAGE Journals scraper### **Fase 4: Requerimiento 4 - Clustering JerÃ¡rquico** (5 dÃ­as)

**Estimado:** 2 dÃ­as (12 horas)

- [ ] ScienceDirect scraper- DÃ­as 17-18: Preprocesamiento de texto y vectorizaciÃ³n

#### **Requerimiento 6: Despliegue y DocumentaciÃ³n** (0%)

- âŒ DockerizaciÃ³n del proyecto- **Prioridad:** MEDIA | **Estimado:** 12 horas- DÃ­a 19: ImplementaciÃ³n de 3 algoritmos de clustering

- âŒ ConfiguraciÃ³n de CI/CD

- âŒ DocumentaciÃ³n tÃ©cnica completa- DÃ­a 20: GeneraciÃ³n de dendrogramas interactivos con D3.js

- âŒ Despliegue en producciÃ³n

---- DÃ­a 21: EvaluaciÃ³n comparativa (Silhouette Score)

**Estimado:** 2 dÃ­as (12 horas)



---

## ğŸ§¹ LIMPIEZA REALIZADA (23 de Enero, 2025)### **Fase 5: Requerimiento 5 - Visualizaciones** (4 dÃ­as)

## ğŸ—“ï¸ PLAN DE ACCIÃ“N PRIORIZADO

- DÃ­a 22: Mapa de calor geogrÃ¡fico (Plotly/Leaflet)

### **Fase 1: Completar Requerimiento 1** (2 dÃ­as)

**Prioridad: CRÃTICA** - Es la base para todos los demÃ¡s requerimientos### **Archivos Eliminados**- DÃ­a 23: Nube de palabras dinÃ¡mica (D3.js)



1. **DÃ­a 1: Completar Scrapers**- DÃ­a 24: LÃ­nea temporal por aÃ±o y revista (Recharts)

   - âœ… Verificar y completar ACM Scraper

   - âœ… Verificar y completar SAGE Scraper#### **Debug y Temporales**- DÃ­a 25: ExportaciÃ³n a PDF (jsPDF + html2canvas)

   - ğŸ†• Implementar ScienceDirect Scraper

   - âœ… Testing de scrapers individuales- âŒ `Backend/debug_bibtex.py` - Script de debug temporal



2. **DÃ­a 2: IntegraciÃ³n y API**- âŒ `Backend/test_debug.py` - Test de debug### **Fase 6: Requerimiento 6 - Despliegue y DocumentaciÃ³n** (4 dÃ­as)

   - ğŸ†• Crear endpoints API `/api/v1/data`

   - âœ… Integrar UnifiedDownloader con todos los scrapers- âŒ `Backend/test_requerimiento1.log` - Log temporal- DÃ­a 26: DocumentaciÃ³n tÃ©cnica de arquitectura

   - âœ… Implementar sistema de jobs en background (Celery/BackgroundTasks)

   - âœ… Testing de integraciÃ³n completa- âŒ `SESION_RESUMEN.md` - DocumentaciÃ³n temporal- DÃ­a 27: DocumentaciÃ³n de algoritmos implementados

   - ğŸ“ Generar dataset unificado de prueba

- âŒ `SESION_PARSERS.md` - DocumentaciÃ³n temporal- DÃ­a 28: DockerizaciÃ³n y CI/CD

**Entregables:**

- Sistema automÃ¡tico de descarga funcionando- âŒ `PROGRESO.md` - Duplicado (se mantiene PROGRESO_ACTUALIZADO.md)- DÃ­a 29: Despliegue en producciÃ³n

- API REST para descarga (`/download`, `/status/{job_id}`, `/unified`, `/duplicates`)

- Dataset unificado en `data/downloads/unified.json`

- Reporte de duplicados en `data/downloads/duplicates_report.json`

#### **Cache y Builds**### **Fase 7: Testing Final y OptimizaciÃ³n** (2 dÃ­as)

---

- âŒ `Backend/__pycache__/` (recursivo)- DÃ­a 30: Testing integral y correcciones

### **Fase 2: Requerimiento 3 - AnÃ¡lisis de Frecuencias** (1.5 dÃ­as)

- âŒ `Backend/.pytest_cache/`- DÃ­a 31: OptimizaciÃ³n de rendimiento

1. **Paso 1: AnÃ¡lisis de CategorÃ­a Predefinida** (0.5 dÃ­a)

   - Implementar `FrequencyAnalyzer` en `app/services/analytics/`- âŒ Todos los `__pycache__/` en subdirectorios

   - Buscar palabras asociadas en abstracts

   - Calcular frecuencias y generar estadÃ­sticas**TOTAL ESTIMADO: 31 dÃ­as (â‰ˆ 6-7 semanas)**



2. **Paso 2: GeneraciÃ³n AutomÃ¡tica de Palabras** (0.5 dÃ­a)#### **Datos Temporales**

   - Usar NLP (NLTK/spaCy) para extracciÃ³n de tÃ©rminos

   - LematizaciÃ³n y eliminaciÃ³n de stopwords- âŒ `Backend/data/downloads/*.json` - Descargas de prueba---

   - Generar top 15 palabras asociadas automÃ¡ticamente

- âŒ `Backend/data/downloads/*.bib`

3. **Paso 3: MÃ©tricas de PrecisiÃ³n** (0.5 dÃ­a)

   - Calcular precisiÃ³n comparando palabras generadas vs predefinidas- âŒ `Backend/data/downloads/*.ris`## ğŸ—ï¸ ARQUITECTURA DEL SISTEMA

   - Implementar endpoints API

   - Testing y visualizaciÃ³n bÃ¡sica- âŒ `Backend/data/downloads/*.csv`



**Entregables:**```

- `app/services/analytics/frequency_analyzer.py`

- Endpoints: `/api/v1/analytics/frequencies`, `/api/v1/analytics/associated-words`**Total eliminado:** ~35 archivos + directorios de cacheâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

- Reporte de precisiÃ³n

â”‚                      FRONTEND (React + TS)                   â”‚

---

### **Nuevo .gitignore Creado**â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚

### **Fase 3: Requerimiento 4 - Clustering JerÃ¡rquico** (2 dÃ­as)

â”‚  â”‚   D3.js     â”‚  â”‚  Plotly.js  â”‚  â”‚  Recharts   â”‚         â”‚

1. **Paso 1: Preprocesamiento** (0.5 dÃ­a)

   - Pipeline de limpieza de textoExcluye:â”‚  â”‚ Dendrogramasâ”‚  â”‚ Mapas Calor â”‚  â”‚  LÃ­neas     â”‚         â”‚

   - VectorizaciÃ³n (TF-IDF o embeddings)

   - NormalizaciÃ³n- `__pycache__/`, `*.pyc`, `*.pyo`â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚



2. **Paso 2: Algoritmos de Clustering** (1 dÃ­a)- `venv/`, `.venv/`, `env/`â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   - Implementar Ward Linkage

   - Implementar Average Linkage- `.pytest_cache/`, `.coverage`                         â”‚ HTTP/REST API

   - Implementar Complete Linkage

   - MÃ©tricas de evaluaciÃ³n (Silhouette Score)- `.env`, `.env.local`â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



3. **Paso 3: Dendrogramas** (0.5 dÃ­a)- `Backend/data/downloads/*` (excepto `.gitkeep`)â”‚                   BACKEND (FastAPI)                          â”‚

   - Generar dendrogramas con scipy/matplotlib

   - Implementar endpoints API- `Backend/logs/*.log`â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚

   - Testing

- `Backend/models/*.pkl`, `*.h5`, `*.pt`â”‚  â”‚            API ENDPOINTS (v1)                        â”‚   â”‚

**Entregables:**

- `app/services/ml_analysis/clustering/hierarchical_clustering.py`- `SESION_*.md` (archivos de sesiÃ³n temporal)â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚

- Endpoints: `/api/v1/clustering/hierarchical`

- Dendrogramas en formato JSON para visualizaciÃ³n- `debug_*.py`, `test_debug.py`â”‚  â”‚ /api/v1/data          - Data Acquisition Service    â”‚   â”‚



---- `node_modules/`, `dist/`, `build/`â”‚  â”‚ /api/v1/ml            - ML & NLP Service             â”‚   â”‚



### **Fase 4: Requerimiento 5 - Visualizaciones** (2 dÃ­as)- `.DS_Store`, `Thumbs.db`â”‚  â”‚ /api/v1/analytics     - Analytics Service            â”‚   â”‚



1. **Backend: Endpoints de Datos** (0.5 dÃ­a)â”‚  â”‚ /api/v1/viz           - Visualization Service        â”‚   â”‚

   - `/api/v1/viz/geographic-distribution`

   - `/api/v1/viz/word-cloud-data`---â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚

   - `/api/v1/viz/timeline`

â”‚                                                              â”‚

2. **Frontend: ImplementaciÃ³n de Visualizaciones** (1 dÃ­a)

   - Mapa de calor geogrÃ¡fico (Plotly/Leaflet)## ğŸ—ï¸ ESTRUCTURA FINAL DEL REPOSITORIOâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚

   - Nube de palabras dinÃ¡mica (D3.js/react-wordcloud)

   - LÃ­nea temporal (Recharts)â”‚  â”‚         SERVICIOS DE NEGOCIO                        â”‚    â”‚



3. **ExportaciÃ³n PDF** (0.5 dÃ­a)```â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚

   - Implementar jsPDF + html2canvas

   - Endpoint `/api/v1/viz/export-pdf`ProyectoAnalisisAlgoritmos/â”‚  â”‚ ğŸ” DataAcquisitionService                           â”‚    â”‚



**Entregables:**â”œâ”€â”€ .gitignore                      # Control de versionesâ”‚  â”‚    - Scrapers (ACM, SAGE, ScienceDirect)           â”‚    â”‚

- Componentes React en `Frontend/bibliometric-app/src/components/visualizations/`

- Sistema de exportaciÃ³n PDF funcionalâ”œâ”€â”€ README.md                       # DocumentaciÃ³n principalâ”‚  â”‚    - UnificaciÃ³n y deduplicaciÃ³n                    â”‚    â”‚



---â”œâ”€â”€ PLAN_IMPLEMENTACION.md         # Este archivoâ”‚  â”‚                                                      â”‚    â”‚



### **Fase 5: Requerimiento 6 - Despliegue** (1.5 dÃ­as)â”œâ”€â”€ PROGRESO_ACTUALIZADO.md        # Estado detallado del proyectoâ”‚  â”‚ ğŸ¤– MLAnalysisService                                â”‚    â”‚



1. **DockerizaciÃ³n** (0.5 dÃ­a)â”‚â”‚  â”‚    - Similitud clÃ¡sica (Levenshtein, TF-IDF, etc.) â”‚    â”‚

   - Dockerfile para Backend

   - Dockerfile para Frontendâ”œâ”€â”€ Backend/â”‚  â”‚    - Similitud IA (BERT, Sentence-BERT)            â”‚    â”‚

   - docker-compose.yml

â”‚   â”œâ”€â”€ main.py                    # AplicaciÃ³n FastAPI principalâ”‚  â”‚    - Clustering jerÃ¡rquico (Ward, Average, etc.)   â”‚    â”‚

2. **DocumentaciÃ³n** (0.5 dÃ­a)

   - README.md completoâ”‚   â”œâ”€â”€ requirements.txt           # Dependencias Pythonâ”‚  â”‚                                                      â”‚    â”‚

   - DocumentaciÃ³n tÃ©cnica de algoritmos

   - GuÃ­a de instalaciÃ³n y usoâ”‚   â”œâ”€â”€ pytest.ini                 # ConfiguraciÃ³n de pytestâ”‚  â”‚ ğŸ“Š AnalyticsService                                 â”‚    â”‚



3. **Despliegue** (0.5 dÃ­a)â”‚   â”œâ”€â”€ .env.example              # Template de variables de entornoâ”‚  â”‚    - Frecuencias de conceptos                       â”‚    â”‚

   - Configurar CI/CD (GitHub Actions)

   - Deploy en servidor (Heroku/Railway/DigitalOcean)â”‚   â”‚â”‚  â”‚    - MÃ©tricas bibliomÃ©tricas                        â”‚    â”‚

   - Testing en producciÃ³n

â”‚   â”œâ”€â”€ app/â”‚  â”‚    - GeneraciÃ³n de palabras asociadas               â”‚    â”‚

**Entregables:**

- AplicaciÃ³n desplegada y accesibleâ”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚  â”‚                                                      â”‚    â”‚

- DocumentaciÃ³n completa

- Repositorio limpio y profesionalâ”‚   â”‚   â”œâ”€â”€ api/â”‚  â”‚ ğŸ“ˆ VisualizationService                             â”‚    â”‚



---â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚  â”‚    - Mapas de calor geogrÃ¡ficos                     â”‚    â”‚



## ğŸ—ï¸ ARQUITECTURA DEL SISTEMAâ”‚   â”‚   â”‚   â””â”€â”€ v1/â”‚  â”‚    - Nubes de palabras                              â”‚    â”‚



```â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.pyâ”‚  â”‚    - LÃ­neas temporales                              â”‚    â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                  FRONTEND (React + TypeScript)               â”‚â”‚   â”‚   â”‚       â”œâ”€â”€ data_acquisition.pyâ”‚  â”‚    - ExportaciÃ³n PDF                                â”‚    â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚

â”‚  â”‚   D3.js      â”‚  â”‚  Plotly.js   â”‚  â”‚  Recharts    â”‚      â”‚â”‚   â”‚   â”‚       â””â”€â”€ similarity.pyâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚

â”‚  â”‚ Dendrogramas â”‚  â”‚ Mapa Calor   â”‚  â”‚  Timeline    â”‚      â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚â”‚   â”‚   â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            â”‚ HTTP/REST APIâ”‚   â”‚   â”œâ”€â”€ config/                         â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                   BACKEND (FastAPI)                          â”‚â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

â”‚  â”‚         API ENDPOINTS (/api/v1)                        â”‚ â”‚â”‚   â”‚   â”‚   â””â”€â”€ settings.pyâ”‚               CAPA DE DATOS                                  â”‚

â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚

â”‚  â”‚ /data/*          - Descarga y unificaciÃ³n (Req 1)     â”‚ â”‚â”‚   â”‚   â”‚â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚

â”‚  â”‚ /similarity/*    - Similitud textual (Req 2) âœ…       â”‚ â”‚

â”‚  â”‚ /analytics/*     - Frecuencias (Req 3)                â”‚ â”‚â”‚   â”‚   â”œâ”€â”€ models/â”‚  â”‚  PostgreSQL  â”‚  â”‚    Redis     â”‚  â”‚  File System â”‚      â”‚

â”‚  â”‚ /clustering/*    - Clustering jerÃ¡rquico (Req 4)      â”‚ â”‚

â”‚  â”‚ /viz/*           - Visualizaciones (Req 5)            â”‚ â”‚â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚  â”‚  Metadatos   â”‚  â”‚    Cache     â”‚  â”‚  Archivos    â”‚      â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

â”‚                                                              â”‚â”‚   â”‚   â”‚   â””â”€â”€ publication.pyâ”‚  â”‚  Publicac.   â”‚  â”‚  Embeddings  â”‚  â”‚  CSV/BibTeX  â”‚      â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

â”‚  â”‚              SERVICIOS DE NEGOCIO                      â”‚ â”‚â”‚   â”‚   â”‚â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚

â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚

â”‚  â”‚ ğŸ“¥ DataAcquisitionService (Req 1) - 40% â³            â”‚ â”‚â”‚   â”‚   â”œâ”€â”€ services/â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚  â”‚    - Scrapers: ACM, SAGE, ScienceDirect              â”‚ â”‚

â”‚  â”‚    - Parsers: BibTeX, RIS, CSV âœ…                     â”‚ â”‚â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py```

â”‚  â”‚    - Deduplicador inteligente                         â”‚ â”‚

â”‚  â”‚                                                        â”‚ â”‚â”‚   â”‚   â”‚   â”œâ”€â”€ analytics/               # Placeholder (Req. 3)

â”‚  â”‚ ğŸ¤– MLAnalysisService                                  â”‚ â”‚

â”‚  â”‚    - Similitud (Req 2) âœ…                             â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â””â”€â”€ __init__.py---

â”‚  â”‚    - Clustering (Req 4) - 0% âŒ                       â”‚ â”‚

â”‚  â”‚                                                        â”‚ â”‚â”‚   â”‚   â”‚   â”œâ”€â”€ data_acquisition/

â”‚  â”‚ ğŸ“Š AnalyticsService (Req 3) - 0% âŒ                   â”‚ â”‚

â”‚  â”‚    - Frecuencias de conceptos                         â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py## ğŸ“ REQUERIMIENTO 1: AUTOMATIZACIÃ“N DE DESCARGA DE DATOS

â”‚  â”‚    - GeneraciÃ³n de palabras asociadas                 â”‚ â”‚

â”‚  â”‚    - MÃ©tricas de precisiÃ³n                            â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base_scraper.py

â”‚  â”‚                                                        â”‚ â”‚

â”‚  â”‚ ğŸ“ˆ VisualizationService (Req 5) - 0% âŒ               â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ crossref_scraper.py### **Objetivo**

â”‚  â”‚    - Mapas de calor geogrÃ¡ficos                       â”‚ â”‚

â”‚  â”‚    - Nubes de palabras dinÃ¡micas                      â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ deduplicator.pyAutomatizar la descarga de publicaciones cientÃ­ficas desde ACM, SAGE y ScienceDirect, unificar los datos en un archivo Ãºnico sin duplicados, y generar un reporte de productos eliminados.

â”‚  â”‚    - LÃ­neas temporales                                â”‚ â”‚

â”‚  â”‚    - ExportaciÃ³n PDF                                  â”‚ â”‚â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ unified_downloader.py

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚   â”‚   â”‚   â””â”€â”€ parsers/### **Componentes a Implementar**

                            â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py

â”‚                   CAPA DE DATOS                              â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ bibtex_parser.py   # âœ… 450 lÃ­neas#### 1.1. **Scrapers y Conectores API**

â”‚  â”‚  PostgreSQL  â”‚  â”‚    Redis     â”‚  â”‚  File System â”‚      â”‚

â”‚  â”‚  (Futuro)    â”‚  â”‚    Cache     â”‚  â”‚  JSON/CSV    â”‚      â”‚â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ris_parser.py      # âœ… 420 lÃ­neas

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ csv_parser.py      # âœ… 290 lÃ­neas**Archivos a crear:**

```

â”‚   â”‚   â”‚   â”‚       â””â”€â”€ unifier.py         # âœ… 220 lÃ­neas```

---

â”‚   â”‚   â”‚   â”œâ”€â”€ ml_analysis/Backend/app/services/data_acquisition/

## ğŸ“¦ ESTRUCTURA DE ARCHIVOS

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”œâ”€â”€ __init__.py

```

ProyectoAnalisisAlgoritmos/â”‚   â”‚   â”‚   â”‚   â””â”€â”€ similarity/â”œâ”€â”€ base_scraper.py          # Clase base abstracta

â”œâ”€â”€ .gitignore                          âœ…

â”œâ”€â”€ README.md                           âœ…â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.pyâ”œâ”€â”€ acm_scraper.py           # Scraper para ACM Digital Library

â”œâ”€â”€ PLAN_IMPLEMENTACION.md             âœ… (este archivo)

â”‚â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ base_similarity.py       # âœ…â”œâ”€â”€ sage_scraper.py          # Scraper para SAGE Publications

â”œâ”€â”€ Backend/

â”‚   â”œâ”€â”€ main.py                        âœ… FastAPI app (247 lÃ­neas)â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ levenshtein.py          # âœ…â”œâ”€â”€ sciencedirect_scraper.py # Scraper para ScienceDirect

â”‚   â”œâ”€â”€ requirements.txt               âœ… 79 dependencias

â”‚   â”œâ”€â”€ pytest.ini                     âœ…â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ tfidf_cosine.py         # âœ…â”œâ”€â”€ unified_downloader.py    # Orquestador de descargas

â”‚   â”‚

â”‚   â”œâ”€â”€ app/â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ jaccard.py              # âœ…â””â”€â”€ parsers/

â”‚   â”‚   â”œâ”€â”€ __init__.py               âœ…

â”‚   â”‚   â”‚â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ngrams.py               # âœ…    â”œâ”€â”€ __init__.py

â”‚   â”‚   â”œâ”€â”€ api/v1/

â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py           âœ…â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ bert_embeddings.py      # âœ…    â”œâ”€â”€ bibtex_parser.py     # Parser para formato BibTex

â”‚   â”‚   â”‚   â”œâ”€â”€ similarity.py         âœ… (742 lÃ­neas) - Req 2

â”‚   â”‚   â”‚   â”œâ”€â”€ data_acquisition.py   â³ (pendiente completar) - Req 1â”‚   â”‚   â”‚   â”‚       â””â”€â”€ sentence_bert.py        # âœ…    â”œâ”€â”€ ris_parser.py        # Parser para formato RIS

â”‚   â”‚   â”‚   â”œâ”€â”€ analytics.py          âŒ (crear) - Req 3

â”‚   â”‚   â”‚   â”œâ”€â”€ clustering.py         âŒ (crear) - Req 4â”‚   â”‚   â”‚   â””â”€â”€ visualization/           # Placeholder    â””â”€â”€ csv_parser.py        # Parser para formato CSV

â”‚   â”‚   â”‚   â””â”€â”€ visualization.py      âŒ (crear) - Req 5

â”‚   â”‚   â”‚â”‚   â”‚   â”‚       â””â”€â”€ __init__.py```

â”‚   â”‚   â”œâ”€â”€ models/

â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py           âœ…â”‚   â”‚   â””â”€â”€ utils/

â”‚   â”‚   â”‚   â””â”€â”€ publication.py        âœ…

â”‚   â”‚   â”‚â”‚   â”‚       â””â”€â”€ __init__.py**Funcionalidades clave:**

â”‚   â”‚   â”œâ”€â”€ services/

â”‚   â”‚   â”‚   â”œâ”€â”€ data_acquisition/â”‚   â”‚

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base_scraper.py           âœ… (368 lÃ­neas)

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ crossref_scraper.py       â³ (verificar)â”‚   â”œâ”€â”€ data/```python

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ acm_scraper.py            â³ (completar)

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sage_scraper.py           â³ (completar)â”‚   â”‚   â””â”€â”€ downloads/# base_scraper.py - Clase base abstracta

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sciencedirect_scraper.py  âŒ (crear)

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ deduplicator.py           â³ (verificar)â”‚   â”‚       â””â”€â”€ .gitkeepfrom abc import ABC, abstractmethod

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ unified_downloader.py     âœ… (431 lÃ­neas)

â”‚   â”‚   â”‚   â”‚   â””â”€â”€ parsers/â”‚   â”‚from typing import List, Dict, Any

â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ bibtex_parser.py      âœ… (450 lÃ­neas)

â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ris_parser.py         âœ… (420 lÃ­neas)â”‚   â”œâ”€â”€ logs/

â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ csv_parser.py         âœ… (290 lÃ­neas)

â”‚   â”‚   â”‚   â”‚       â””â”€â”€ unifier.py            âœ… (220 lÃ­neas)â”‚   â”‚   â””â”€â”€ .gitkeepclass BaseScraper(ABC):

â”‚   â”‚   â”‚   â”‚

â”‚   â”‚   â”‚   â”œâ”€â”€ ml_analysis/â”‚   â”‚    """Clase base para todos los scrapers de bases de datos cientÃ­ficas."""

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ similarity/                    âœ… Req 2

â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ levenshtein.py            âœ…â”‚   â”œâ”€â”€ models/    

â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tfidf_cosine.py           âœ…

â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ jaccard.py                âœ…â”‚   â”‚   â””â”€â”€ .gitkeep    @abstractmethod

â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ngrams.py                 âœ…

â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ bert_embeddings.py        âœ…â”‚   â”‚    async def search(self, query: str, max_results: int) -> List[Dict[str, Any]]:

â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ sentence_bert.py          âœ…

â”‚   â”‚   â”‚   â”‚   â”‚â”‚   â””â”€â”€ tests/        """Busca publicaciones con la query especificada."""

â”‚   â”‚   â”‚   â”‚   â””â”€â”€ clustering/                    âŒ Req 4

â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py               âŒ (crear)â”‚       â”œâ”€â”€ __init__.py        pass

â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ hierarchical.py           âŒ (crear)

â”‚   â”‚   â”‚   â”‚       â””â”€â”€ dendrogram_generator.py   âŒ (crear)â”‚       â”œâ”€â”€ test_similarity_endpoints.py   # âœ… 23 tests    

â”‚   â”‚   â”‚   â”‚

â”‚   â”‚   â”‚   â”œâ”€â”€ analytics/                         âŒ Req 3â”‚       â”œâ”€â”€ test_similarity_api.py         # âœ… 3 tests    @abstractmethod

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py                   âœ… (placeholder)

â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ frequency_analyzer.py         âŒ (crear)â”‚       â”œâ”€â”€ test_parsers.py                # âœ… 7 tests    async def download_metadata(self, publication_id: str) -> Dict[str, Any]:

â”‚   â”‚   â”‚   â”‚   â””â”€â”€ word_extractor.py             âŒ (crear)

â”‚   â”‚   â”‚   â”‚â”‚       â”œâ”€â”€ test_similitud.py              # âœ… 4 tests        """Descarga metadatos completos de una publicaciÃ³n."""

â”‚   â”‚   â”‚   â””â”€â”€ visualization/                     âŒ Req 5

â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py                   âœ… (placeholder)â”‚       â””â”€â”€ test_requerimiento1.py         # âœ… 2 tests        pass

â”‚   â”‚   â”‚       â”œâ”€â”€ geographic_map.py             âŒ (crear)

â”‚   â”‚   â”‚       â””â”€â”€ pdf_exporter.py               âŒ (crear)â”‚    

â”‚   â”‚   â”‚

â”‚   â”‚   â””â”€â”€ utils/â””â”€â”€ Frontend/    @abstractmethod

â”‚   â”‚       â””â”€â”€ __init__.py                       âœ…

â”‚   â”‚    â””â”€â”€ bibliometric-app/    def export_to_format(self, publications: List[Dict], format: str) -> str:

â”‚   â”œâ”€â”€ data/downloads/                           âœ… (.gitkeep)

â”‚   â”œâ”€â”€ logs/                                     âœ… (.gitkeep)        â”œâ”€â”€ package.json        """Exporta publicaciones al formato especificado (bibtex, ris, csv)."""

â”‚   â”œâ”€â”€ models/                                   âœ… (.gitkeep)

â”‚   â”‚        â”œâ”€â”€ vite.config.ts        pass

â”‚   â””â”€â”€ tests/

â”‚       â”œâ”€â”€ test_parsers.py                      âœ… (7 tests pasando)        â”œâ”€â”€ tsconfig.json```

â”‚       â”œâ”€â”€ test_similitud.py                    âœ… (4 tests pasando)

â”‚       â”œâ”€â”€ test_similarity_api.py               âœ… (3 tests pasando)        â”œâ”€â”€ .gitignore

â”‚       â”œâ”€â”€ test_similarity_endpoints.py         âœ… (23 tests pasando)

â”‚       â””â”€â”€ test_requerimiento1.py               âœ… (2 tests pasando)        â”œâ”€â”€ public/**TecnologÃ­as:**

â”‚

â””â”€â”€ Frontend/bibliometric-app/        â””â”€â”€ src/- `scrapy` para scraping web

    â”œâ”€â”€ package.json                             âœ…

    â”œâ”€â”€ vite.config.ts                           âœ…            â”œâ”€â”€ main.tsx- `aiohttp` para peticiones asÃ­ncronas

    â””â”€â”€ src/

        â”œâ”€â”€ main.tsx                             âœ…            â”œâ”€â”€ App.tsx- `crossref-commons` para API de CrossRef

        â”œâ”€â”€ App.tsx                              âœ…

        â”œâ”€â”€ components/                          â³ (pendiente)            â”œâ”€â”€ types/- `scholarly` para bÃºsquedas acadÃ©micas

        â””â”€â”€ services/api.ts                      âœ…

```            â”œâ”€â”€ services/- `habanero` para API de CrossRef



---            â””â”€â”€ hooks/- `beautifulsoup4` para parsing HTML



## âœ… CHECKLIST DE FINALIZACIÃ“N```



### Requerimiento 1: AutomatizaciÃ³n de Descarga#### 1.2. **Sistema de UnificaciÃ³n y DeduplicaciÃ³n**

- [x] Parsers BibTeX, RIS, CSV âœ…

- [x] BaseScraper y estructura âœ…**EstadÃ­sticas:**

- [ ] ACM Scraper funcional

- [ ] SAGE Scraper funcional- **Archivos Python activos:** 35**Archivos a crear:**

- [ ] ScienceDirect Scraper funcional

- [ ] Deduplicador probado- **LÃ­neas de cÃ³digo:** ~4,500```

- [ ] UnifiedDownloader integrado

- [ ] Endpoints API `/api/v1/data/*`- **Tests:** 39 totales (11 pasando)Backend/app/services/data_acquisition/

- [ ] Testing completo

- [ ] Dataset unificado generado- **Cobertura:** ~70%â”œâ”€â”€ deduplicator.py          # Sistema de eliminaciÃ³n de duplicados

- [ ] Reporte de duplicados generado

â”œâ”€â”€ unifier.py               # UnificaciÃ³n de formatos

### Requerimiento 2: Similitud Textual

- [x] 6 algoritmos implementados âœ…---â””â”€â”€ models/

- [x] DocumentaciÃ³n matemÃ¡tica âœ…

- [x] Endpoints API âœ…    â”œâ”€â”€ publication.py       # Modelo de datos unificado

- [x] Testing completo âœ…

- [x] UI para comparaciÃ³n âœ…## ğŸ“¦ DEPENDENCIAS    â””â”€â”€ duplicate_report.py  # Modelo de reporte de duplicados



### Requerimiento 3: Frecuencias```

- [ ] FrequencyAnalyzer

- [ ] AnÃ¡lisis de categorÃ­a predefinida### **Backend (Python 3.13)**

- [ ] GeneraciÃ³n de palabras con NLP

- [ ] MÃ©tricas de precisiÃ³n**Algoritmo de deduplicaciÃ³n:**

- [ ] Endpoints API

- [ ] Visualizaciones```txt



### Requerimiento 4: Clustering# Web Framework```python

- [ ] 3 algoritmos de clustering

- [ ] Preprocesamiento de textofastapi==0.119.1# deduplicator.py

- [ ] GeneraciÃ³n de dendrogramas

- [ ] EvaluaciÃ³n (Silhouette Score)uvicorn==0.34.0import hashlib

- [ ] Endpoints API

- [ ] Testingpydantic==2.10.3from typing import List, Tuple, Dict



### Requerimiento 5: Visualizacionesfrom difflib import SequenceMatcher

- [ ] Mapa de calor geogrÃ¡fico

- [ ] Nube de palabras dinÃ¡mica# ML/NLP

- [ ] LÃ­nea temporal

- [ ] ExportaciÃ³n PDFscikit-learn==1.6.1class Deduplicator:

- [ ] Endpoints API

- [ ] IntegraciÃ³n frontendsentence-transformers==3.4.0    """Sistema inteligente de eliminaciÃ³n de duplicados."""



### Requerimiento 6: Desplieguepython-Levenshtein==0.27.0    

- [ ] Dockerfile Backend

- [ ] Dockerfile Frontend    def __init__(self, similarity_threshold: float = 0.95):

- [ ] docker-compose.yml

- [ ] CI/CD configurado# Testing        self.similarity_threshold = similarity_threshold

- [ ] DocumentaciÃ³n completa

- [ ] AplicaciÃ³n desplegadapytest==8.4.2        self.duplicates_report = []



---pytest-asyncio==0.25.2    



## ğŸ“ˆ PROGRESO GENERALhttpx==0.28.1    def calculate_title_similarity(self, title1: str, title2: str) -> float:



``````        """Calcula similitud entre tÃ­tulos usando SequenceMatcher."""

Requerimiento 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 40%

Requerimiento 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% âœ…        return SequenceMatcher(None, title1.lower(), title2.lower()).ratio()

Requerimiento 3: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]   0%

Requerimiento 4: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]   0%### **Frontend (Node.js)**    

Requerimiento 5: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]   0%

Requerimiento 6: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]   0%    def generate_hash(self, publication: Dict) -> str:



TOTAL PROYECTO: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 40%```json        """Genera hash Ãºnico basado en tÃ­tulo y DOI."""

```

{        identifier = f"{publication.get('title', '')}_{publication.get('doi', '')}"

**Estimado para completar:** 9 dÃ­as (54 horas de trabajo)

  "dependencies": {        return hashlib.md5(identifier.encode()).hexdigest()

---

    "react": "^18.3.1",    

## ğŸš€ PRÃ“XIMOS PASOS INMEDIATOS

    "@mui/material": "^6.3.0",    def deduplicate(

1. **Verificar estado de scrapers existentes**

   - Revisar `acm_scraper.py`, `sage_scraper.py`, `crossref_scraper.py`    "recharts": "^2.15.0"        self, 

   - Identificar quÃ© falta completar

  }        publications: List[Dict]

2. **Completar Requerimiento 1**

   - Implementar scrapers faltantes}    ) -> Tuple[List[Dict], List[Dict]]:

   - Crear endpoints API

   - Testing completo```        """

   - Generar dataset de prueba

        Elimina duplicados y retorna:

3. **Continuar con Requerimiento 3**

   - Implementar anÃ¡lisis de frecuencias---        - Lista de publicaciones Ãºnicas

   - NLP para generaciÃ³n de palabras

        - Lista de duplicados eliminados

---

## ğŸ¯ PRÃ“XIMOS PASOS        """

**Ãšltima actualizaciÃ³n:** 23 de Enero, 2025  

**VersiÃ³n:** 3.0          unique_publications = []

**Estado:** ğŸŸ¢ En desarrollo activo (40% completado)

### **Inmediato**        duplicates = []

1. **Requerimiento 3:** AnÃ¡lisis de Frecuencias (6h)        seen_hashes = set()

2. **Requerimiento 4:** Clustering JerÃ¡rquico (6h)        

3. **Frontend:** IntegraciÃ³n bÃ¡sica (8h)        for pub in publications:

            pub_hash = self.generate_hash(pub)

### **Corto Plazo**            

4. **Requerimiento 5:** Scrapers adicionales (12h)            # Verificar duplicados exactos por hash

5. **Visualizaciones:** ImplementaciÃ³n completa (10h)            if pub_hash in seen_hashes:

                duplicates.append(pub)

### **Largo Plazo**                continue

6. **Despliegue:** ProducciÃ³n (4h)            

7. **DocumentaciÃ³n:** Completa (8h)            # Verificar duplicados por similitud de tÃ­tulo

            is_duplicate = False

---            for unique_pub in unique_publications:

                similarity = self.calculate_title_similarity(

## âœ… CHECKLIST DE CALIDAD                    pub.get('title', ''),

                    unique_pub.get('title', '')

- [x] CÃ³digo limpio y documentado                )

- [x] Tests pasando                if similarity >= self.similarity_threshold:

- [x] .gitignore configurado                    duplicates.append(pub)

- [x] Estructura modular                    is_duplicate = True

- [x] Logging implementado                    break

- [ ] Cobertura >80%            

- [ ] Frontend funcional            if not is_duplicate:

- [ ] CI/CD configurado                unique_publications.append(pub)

                seen_hashes.add(pub_hash)

---        

        return unique_publications, duplicates

**Ãšltima actualizaciÃ³n:** 23 de Enero, 2025  ```

**VersiÃ³n:** 2.0  

**Estado:** ğŸŸ¢ En desarrollo activo#### 1.3. **Endpoints API**


**Archivos a crear:**
```
Backend/app/api/v1/
â”œâ”€â”€ __init__.py
â””â”€â”€ data_acquisition.py  # Endpoints para descarga y unificaciÃ³n
```

**Endpoints a implementar:**

```python
# data_acquisition.py
from fastapi import APIRouter, BackgroundTasks, HTTPException
from typing import List, Optional
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/data", tags=["Data Acquisition"])

class DownloadRequest(BaseModel):
    query: str
    sources: List[str]  # ['acm', 'sage', 'sciencedirect']
    max_results_per_source: int = 100
    export_format: str = 'json'  # json, bibtex, ris, csv

class DownloadResponse(BaseModel):
    job_id: str
    status: str
    message: str

@router.post("/download", response_model=DownloadResponse)
async def start_download(
    request: DownloadRequest,
    background_tasks: BackgroundTasks
):
    """
    Inicia descarga automÃ¡tica desde mÃºltiples fuentes.
    
    - **query**: Cadena de bÃºsqueda (ej: "generative artificial intelligence")
    - **sources**: Lista de bases de datos a consultar
    - **max_results_per_source**: Cantidad mÃ¡xima de resultados por fuente
    - **export_format**: Formato de exportaciÃ³n deseado
    """
    # ImplementaciÃ³n con Celery para tareas en background
    pass

@router.get("/status/{job_id}")
async def get_download_status(job_id: str):
    """Consulta el estado de una descarga en proceso."""
    pass

@router.get("/unified")
async def get_unified_data(
    format: Optional[str] = 'json',
    include_metadata: bool = True
):
    """Obtiene los datos unificados sin duplicados."""
    pass

@router.get("/duplicates")
async def get_duplicates_report():
    """Obtiene el reporte de publicaciones duplicadas eliminadas."""
    pass
```

#### 1.4. **Modelos de Datos**

```python
# Backend/app/models/publication.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import date

class Author(BaseModel):
    name: str
    affiliation: Optional[str] = None
    email: Optional[str] = None
    country: Optional[str] = None

class Publication(BaseModel):
    """Modelo unificado de publicaciÃ³n cientÃ­fica."""
    
    id: str = Field(description="ID Ãºnico generado")
    title: str = Field(description="TÃ­tulo del artÃ­culo")
    abstract: str = Field(description="Resumen del artÃ­culo")
    authors: List[Author] = Field(description="Lista de autores")
    keywords: List[str] = Field(default=[], description="Palabras clave")
    doi: Optional[str] = Field(None, description="DOI del artÃ­culo")
    publication_date: Optional[date] = Field(None, description="Fecha de publicaciÃ³n")
    journal: Optional[str] = Field(None, description="Revista o conferencia")
    source: str = Field(description="Fuente de datos (acm, sage, sciencedirect)")
    url: Optional[str] = Field(None, description="URL del artÃ­culo")
    citation_count: Optional[int] = Field(0, description="NÃºmero de citas")
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "pub_001",
                "title": "Generative AI in Educational Contexts",
                "abstract": "This study explores...",
                "authors": [
                    {
                        "name": "John Doe",
                        "affiliation": "MIT",
                        "country": "USA"
                    }
                ],
                "keywords": ["Generative AI", "Education", "Machine Learning"],
                "doi": "10.1145/example",
                "publication_date": "2024-06-15",
                "journal": "ACM Transactions on Computer Education",
                "source": "acm"
            }
        }
```

#### 1.5. **Testing y ValidaciÃ³n**

**Archivos a crear:**
```
Backend/tests/
â”œâ”€â”€ test_data_acquisition/
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â”œâ”€â”€ test_deduplicator.py
â”‚   â”œâ”€â”€ test_parsers.py
â”‚   â””â”€â”€ test_api_endpoints.py
```

**Tests a implementar:**

```python
# test_deduplicator.py
import pytest
from app.services.data_acquisition.deduplicator import Deduplicator

def test_exact_duplicate_detection():
    """Verifica detecciÃ³n de duplicados exactos."""
    publications = [
        {"title": "AI in Education", "doi": "10.1145/123"},
        {"title": "AI in Education", "doi": "10.1145/123"},  # Duplicado
    ]
    
    dedup = Deduplicator()
    unique, duplicates = dedup.deduplicate(publications)
    
    assert len(unique) == 1
    assert len(duplicates) == 1

def test_similar_title_detection():
    """Verifica detecciÃ³n de duplicados por similitud."""
    publications = [
        {"title": "Generative AI in Education", "doi": "10.1145/123"},
        {"title": "Generative AI in Educational Contexts", "doi": "10.1145/456"},
    ]
    
    dedup = Deduplicator(similarity_threshold=0.8)
    unique, duplicates = dedup.deduplicate(publications)
    
    # DeberÃ­a detectar como duplicados si similitud > 0.8
    assert len(unique) + len(duplicates) == 2
```

---

## ğŸ“ REQUERIMIENTO 2: ALGORITMOS DE SIMILITUD TEXTUAL

### **Objetivo**
Implementar 6 algoritmos de similitud textual (4 clÃ¡sicos + 2 con IA) con documentaciÃ³n matemÃ¡tica detallada y UI para comparaciÃ³n de abstracts.

### **Componentes a Implementar**

#### 2.1. **Algoritmos ClÃ¡sicos de Similitud**

**Archivos a crear:**
```
Backend/app/services/ml_analysis/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ similarity/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_similarity.py        # Clase base abstracta
â”‚   â”œâ”€â”€ levenshtein.py            # Distancia de ediciÃ³n
â”‚   â”œâ”€â”€ tfidf_cosine.py           # TF-IDF + Similitud del Coseno
â”‚   â”œâ”€â”€ jaccard.py                # Coeficiente de Jaccard
â”‚   â”œâ”€â”€ ngrams.py                 # Similitud por N-gramas
â”‚   â”œâ”€â”€ bert_embeddings.py        # BERT Embeddings
â”‚   â”œâ”€â”€ sentence_bert.py          # Sentence-BERT
â”‚   â””â”€â”€ similarity_analyzer.py    # Orquestador de anÃ¡lisis
```

**2.1.1. Distancia de Levenshtein**

```python
# levenshtein.py
import numpy as np
from typing import Tuple, List, Dict

class LevenshteinSimilarity:
    """
    ImplementaciÃ³n de la Distancia de Levenshtein (Edit Distance).
    
    FUNDAMENTO MATEMÃTICO:
    =====================
    La distancia de Levenshtein entre dos cadenas s1 y s2 es el nÃºmero mÃ­nimo
    de operaciones de ediciÃ³n (inserciÃ³n, eliminaciÃ³n, sustituciÃ³n) necesarias
    para transformar s1 en s2.
    
    ALGORITMO DE PROGRAMACIÃ“N DINÃMICA:
    ===================================
    Sea DP[i][j] = distancia entre s1[0...i-1] y s2[0...j-1]
    
    Casos base:
    - DP[0][j] = j (insertar j caracteres)
    - DP[i][0] = i (eliminar i caracteres)
    
    Recurrencia:
    DP[i][j] = {
        DP[i-1][j-1]                           si s1[i-1] == s2[j-1]
        1 + min(DP[i-1][j],                    eliminaciÃ³n
                DP[i][j-1],                    inserciÃ³n
                DP[i-1][j-1])                  sustituciÃ³n
                                               en caso contrario
    }
    
    COMPLEJIDAD:
    - Tiempo: O(m * n) donde m = len(s1), n = len(s2)
    - Espacio: O(m * n) para la matriz DP
    
    SIMILITUD NORMALIZADA:
    similarity = 1 - (distance / max(len(s1), len(s2)))
    """
    
    def __init__(self):
        self.name = "Distancia de Levenshtein"
        self.description = "Medida de similitud basada en operaciones de ediciÃ³n"
    
    def calculate_distance(
        self, 
        text1: str, 
        text2: str,
        return_matrix: bool = False
    ) -> Tuple[int, np.ndarray]:
        """
        Calcula la distancia de Levenshtein usando programaciÃ³n dinÃ¡mica.
        
        Args:
            text1: Primer texto
            text2: Segundo texto
            return_matrix: Si True, retorna la matriz DP completa
        
        Returns:
            Tupla (distancia, matriz_dp)
        """
        m, n = len(text1), len(text2)
        
        # Inicializar matriz DP
        dp = np.zeros((m + 1, n + 1), dtype=int)
        
        # Casos base
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        # Llenar matriz usando programaciÃ³n dinÃ¡mica
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if text1[i-1] == text2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(
                        dp[i-1][j],      # EliminaciÃ³n
                        dp[i][j-1],      # InserciÃ³n
                        dp[i-1][j-1]     # SustituciÃ³n
                    )
        
        distance = dp[m][n]
        return (distance, dp) if return_matrix else (distance, None)
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud normalizada [0, 1].
        
        similarity = 1 - (distance / max_length)
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (idÃ©nticos)
        """
        distance, _ = self.calculate_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1.0 - (distance / max_len)
        return similarity
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        AnÃ¡lisis detallado paso a paso con explicaciÃ³n matemÃ¡tica.
        
        Returns:
            Diccionario con:
            - distance: Distancia de Levenshtein
            - similarity: Similitud normalizada
            - dp_matrix: Matriz de programaciÃ³n dinÃ¡mica
            - operations: Lista de operaciones necesarias
            - explanation: ExplicaciÃ³n paso a paso
        """
        distance, dp_matrix = self.calculate_distance(text1, text2, return_matrix=True)
        similarity = self.calculate_similarity(text1, text2)
        
        # Reconstruir secuencia de operaciones
        operations = self._reconstruct_operations(text1, text2, dp_matrix)
        
        # Generar explicaciÃ³n detallada
        explanation = self._generate_explanation(
            text1, text2, distance, similarity, operations
        )
        
        return {
            "algorithm": self.name,
            "distance": distance,
            "similarity": similarity,
            "dp_matrix": dp_matrix.tolist(),
            "operations": operations,
            "explanation": explanation,
            "complexity": {
                "time": f"O({len(text1)} Ã— {len(text2)}) = O({len(text1) * len(text2)})",
                "space": f"O({len(text1)} Ã— {len(text2)}) = O({len(text1) * len(text2)})"
            }
        }
    
    def _reconstruct_operations(
        self, 
        text1: str, 
        text2: str, 
        dp: np.ndarray
    ) -> List[Dict[str, str]]:
        """Reconstruye la secuencia de operaciones de ediciÃ³n."""
        operations = []
        i, j = len(text1), len(text2)
        
        while i > 0 or j > 0:
            if i == 0:
                operations.append({
                    "type": "insert",
                    "char": text2[j-1],
                    "position": j-1
                })
                j -= 1
            elif j == 0:
                operations.append({
                    "type": "delete",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
            elif text1[i-1] == text2[j-1]:
                operations.append({
                    "type": "match",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
                j -= 1
            else:
                # Determinar operaciÃ³n Ã³ptima
                delete_cost = dp[i-1][j]
                insert_cost = dp[i][j-1]
                substitute_cost = dp[i-1][j-1]
                
                min_cost = min(delete_cost, insert_cost, substitute_cost)
                
                if min_cost == substitute_cost:
                    operations.append({
                        "type": "substitute",
                        "from_char": text1[i-1],
                        "to_char": text2[j-1],
                        "position": i-1
                    })
                    i -= 1
                    j -= 1
                elif min_cost == delete_cost:
                    operations.append({
                        "type": "delete",
                        "char": text1[i-1],
                        "position": i-1
                    })
                    i -= 1
                else:
                    operations.append({
                        "type": "insert",
                        "char": text2[j-1],
                        "position": j-1
                    })
                    j -= 1
        
        return list(reversed(operations))
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        distance: int,
        similarity: float,
        operations: List[Dict]
    ) -> str:
        """Genera explicaciÃ³n matemÃ¡tica detallada."""
        explanation = f"""
        ANÃLISIS DE SIMILITUD CON DISTANCIA DE LEVENSHTEIN
        ==================================================
        
        Textos analizados:
        - Texto 1 (m={len(text1)}): "{text1[:50]}..."
        - Texto 2 (n={len(text2)}): "{text2[:50]}..."
        
        RESULTADOS:
        -----------
        - Distancia de Levenshtein: {distance}
        - Similitud normalizada: {similarity:.4f} ({similarity*100:.2f}%)
        
        OPERACIONES NECESARIAS ({len([op for op in operations if op['type'] != 'match'])} operaciones):
        ---------------------
        """
        
        for i, op in enumerate(operations[:10], 1):  # Primeras 10 operaciones
            if op['type'] == 'substitute':
                explanation += f"{i}. Sustituir '{op['from_char']}' por '{op['to_char']}' en posiciÃ³n {op['position']}\n"
            elif op['type'] == 'insert':
                explanation += f"{i}. Insertar '{op['char']}' en posiciÃ³n {op['position']}\n"
            elif op['type'] == 'delete':
                explanation += f"{i}. Eliminar '{op['char']}' de posiciÃ³n {op['position']}\n"
        
        if len(operations) > 10:
            explanation += f"... ({len(operations) - 10} operaciones mÃ¡s)\n"
        
        return explanation
```

**2.1.2. TF-IDF + Similitud del Coseno**

```python
# tfidf_cosine.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict, List, Tuple

class TFIDFCosineSimilarity:
    """
    ImplementaciÃ³n de Similitud TF-IDF + Coseno.
    
    FUNDAMENTO MATEMÃTICO:
    =====================
    
    1. TF-IDF (Term Frequency - Inverse Document Frequency):
       
       TF(t,d) = Frecuencia del tÃ©rmino t en el documento d
                 -----------------------------------------------
                 NÃºmero total de tÃ©rminos en el documento d
       
       IDF(t,D) = log( NÃºmero total de documentos en D )
                      ------------------------------------
                      NÃºmero de documentos que contienen t
       
       TF-IDF(t,d,D) = TF(t,d) Ã— IDF(t,D)
    
    2. Similitud del Coseno:
       
       cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
       
       Donde:
       - A Â· B = Producto punto de vectores A y B = Î£(Ai Ã— Bi)
       - ||A|| = Norma euclidiana de A = âˆš(Î£(AiÂ²))
       - ||B|| = Norma euclidiana de B = âˆš(Î£(BiÂ²))
       
       Rango: [-1, 1]
       - 1 = Vectores idÃ©nticos en direcciÃ³n
       - 0 = Vectores ortogonales
       - -1 = Vectores opuestos
    
    INTERPRETACIÃ“N:
    - La similitud del coseno mide el Ã¡ngulo entre dos vectores en el espacio
      de caracterÃ­sticas TF-IDF.
    - No considera la magnitud, solo la orientaciÃ³n.
    - Ideal para comparar documentos de diferentes longitudes.
    
    COMPLEJIDAD:
    - VectorizaciÃ³n TF-IDF: O(n Ã— m) donde n = docs, m = vocabulario
    - Similitud del coseno: O(m) por par de documentos
    """
    
    def __init__(
        self, 
        max_features: int = 5000,
        ngram_range: Tuple[int, int] = (1, 2),
        min_df: int = 1,
        max_df: float = 0.9
    ):
        """
        Inicializa el analizador TF-IDF + Coseno.
        
        Args:
            max_features: NÃºmero mÃ¡ximo de caracterÃ­sticas (tÃ©rminos)
            ngram_range: Rango de n-gramas a considerar
            min_df: Frecuencia mÃ­nima de documento
            max_df: Frecuencia mÃ¡xima de documento (fracciÃ³n)
        """
        self.name = "TF-IDF + Similitud del Coseno"
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=min_df,
            max_df=max_df,
            stop_words='english',
            lowercase=True,
            strip_accents='unicode'
        )
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud del coseno entre dos textos.
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (idÃ©nticos)
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return float(similarity)
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        AnÃ¡lisis detallado con vectores TF-IDF y explicaciÃ³n matemÃ¡tica.
        
        Returns:
            Diccionario con anÃ¡lisis completo paso a paso
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Obtener nombres de caracterÃ­sticas
        feature_names = self.vectorizer.get_feature_names_out()
        
        # Extraer vectores TF-IDF
        vector1 = tfidf_matrix[0].toarray()[0]
        vector2 = tfidf_matrix[1].toarray()[0]
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        # Identificar tÃ©rminos mÃ¡s relevantes
        top_terms_text1 = self._get_top_terms(vector1, feature_names, top_n=10)
        top_terms_text2 = self._get_top_terms(vector2, feature_names, top_n=10)
        
        # Calcular normas euclidianas
        norm1 = np.linalg.norm(vector1)
        norm2 = np.linalg.norm(vector2)
        
        # Calcular producto punto
        dot_product = np.dot(vector1, vector2)
        
        # Generar explicaciÃ³n detallada
        explanation = self._generate_explanation(
            text1, text2, vector1, vector2, 
            dot_product, norm1, norm2, similarity,
            top_terms_text1, top_terms_text2
        )
        
        return {
            "algorithm": self.name,
            "similarity": float(similarity),
            "vectors": {
                "text1": {
                    "vector": vector1.tolist(),
                    "norm": float(norm1),
                    "top_terms": top_terms_text1
                },
                "text2": {
                    "vector": vector2.tolist(),
                    "norm": float(norm2),
                    "top_terms": top_terms_text2
                }
            },
            "dot_product": float(dot_product),
            "cosine_angle_degrees": float(np.degrees(np.arccos(similarity))),
            "vocabulary_size": len(feature_names),
            "explanation": explanation,
            "complexity": {
                "vectorization": f"O(n Ã— m) donde n=2 documentos, m={len(feature_names)} tÃ©rminos",
                "similarity": "O(m) para producto punto y normas"
            }
        }
    
    def _get_top_terms(
        self, 
        vector: np.ndarray, 
        feature_names: np.ndarray, 
        top_n: int = 10
    ) -> List[Dict[str, any]]:
        """Extrae los tÃ©rminos con mayor peso TF-IDF."""
        # Obtener Ã­ndices de tÃ©rminos ordenados por peso TF-IDF
        top_indices = np.argsort(vector)[-top_n:][::-1]
        
        top_terms = []
        for idx in top_indices:
            if vector[idx] > 0:
                top_terms.append({
                    "term": feature_names[idx],
                    "tfidf_weight": float(vector[idx])
                })
        
        return top_terms
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        vector1: np.ndarray,
        vector2: np.ndarray,
        dot_product: float,
        norm1: float,
        norm2: float,
        similarity: float,
        top_terms1: List[Dict],
        top_terms2: List[Dict]
    ) -> str:
        """Genera explicaciÃ³n matemÃ¡tica detallada."""
        angle_degrees = np.degrees(np.arccos(np.clip(similarity, -1, 1)))
        
        explanation = f"""
        ANÃLISIS DE SIMILITUD CON TF-IDF + COSENO
        =========================================
        
        Textos analizados:
        - Texto 1: "{text1[:50]}..."
        - Texto 2: "{text2[:50]}..."
        
        PASO 1: VECTORIZACIÃ“N TF-IDF
        -----------------------------
        Vocabulario total: {len(vector1)} tÃ©rminos Ãºnicos
        
        TÃ©rminos mÃ¡s relevantes en Texto 1:
        """
        
        for i, term_info in enumerate(top_terms1[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += "\nTÃ©rminos mÃ¡s relevantes en Texto 2:\n"
        for i, term_info in enumerate(top_terms2[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += f"""
        
        PASO 2: CÃLCULO DE SIMILITUD DEL COSENO
        ----------------------------------------
        FÃ³rmula: cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
        
        Producto punto (A Â· B): {dot_product:.6f}
        Norma de A (||A||): {norm1:.6f}
        Norma de B (||B||): {norm2:.6f}
        
        Similitud del coseno: {dot_product:.6f} / ({norm1:.6f} Ã— {norm2:.6f})
                            = {dot_product:.6f} / {norm1 * norm2:.6f}
                            = {similarity:.6f}
        
        Ãngulo entre vectores: {angle_degrees:.2f}Â°
        
        INTERPRETACIÃ“N:
        ---------------
        - Similitud: {similarity:.4f} ({similarity*100:.2f}%)
        - Ãngulo: {angle_degrees:.2f}Â° (0Â° = idÃ©nticos, 90Â° = ortogonales)
        - Los documentos son {"muy similares" if similarity > 0.8 else "moderadamente similares" if similarity > 0.5 else "poco similares"}
        """
        
        return explanation
```

**(ContinuarÃ© con los otros 4 algoritmos de similitud en el siguiente mensaje debido a la longitud...)**

Ahora voy a crear el resto de los algoritmos de similitud y los siguientes requerimientos de manera estructurada.

---

## ğŸ“¦ PRÃ“XIMOS PASOS A IMPLEMENTAR

Ahora voy a crear la estructura completa del proyecto con todos los archivos necesarios. ComenzarÃ© implementando los componentes paso a paso.

Â¿Deseas que continÃºe con:

1. **ImplementaciÃ³n completa del Requerimiento 1** (Scrapers + DeduplicaciÃ³n)
2. **Completar todos los algoritmos de similitud** (6 algoritmos completos)
3. **ImplementaciÃ³n del Requerimiento 3** (Frecuencias de conceptos)
4. **ImplementaciÃ³n del Requerimiento 4** (Clustering y dendrogramas)
5. **O prefieres que cree primero la estructura completa** y luego implementemos cada mÃ³dulo?

TambiÃ©n puedo:
- Crear scripts de inicializaciÃ³n de base de datos
- Configurar Docker y Docker Compose
- Implementar el frontend con los componentes de visualizaciÃ³n
- Crear tests automatizados

**Â¿Por dÃ³nde quieres que empiece?** Te recomiendo seguir el orden de los requerimientos para tener un flujo de trabajo lÃ³gico.
