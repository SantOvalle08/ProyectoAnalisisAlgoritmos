# üìã PLAN DE IMPLEMENTACI√ìN - PROYECTO BIBLIOM√âTRICO# üìã PLAN DE IMPLEMENTACI√ìN - ACTUALIZADO# üìã PLAN DE IMPLEMENTACI√ìN DETALLADO



**Universidad del Quind√≠o - An√°lisis de Algoritmos (2025-2)**  ## Proyecto de An√°lisis Bibliom√©trico - Universidad del Quind√≠o## Proyecto de An√°lisis Bibliom√©trico - Universidad del Quind√≠o

**Autores:** Santiago Ovalle Cort√©s, Juan Sebasti√°n Nore√±a  

**√öltima actualizaci√≥n:** 23 de Enero, 2025  

**Dominio:** Inteligencia Artificial Generativa en Educaci√≥n  

**Cadena de b√∫squeda:** "generative artificial intelligence"**Autores:** Santiago Ovalle Cort√©s, Juan Sebasti√°n Nore√±a  **Autores:** Santiago Ovalle Cort√©s, Juan Sebasti√°n Nore√±a  



---**Curso:** An√°lisis de Algoritmos (2025-2)  **Curso:** An√°lisis de Algoritmos (2025-2)  



## üéØ OBJETIVO GENERAL**√öltima actualizaci√≥n:** 23 de Enero, 2025  **Fecha de inicio:** 20 de Octubre, 2025  



Implementar una plataforma web automatizada para an√°lisis bibliom√©trico de publicaciones cient√≠ficas sobre IA Generativa, integrando algoritmos cl√°sicos y modernos de ML/NLP, con capacidades de visualizaci√≥n interactiva y exportaci√≥n de resultados.**Dominio:** Inteligencia Artificial Generativa en Educaci√≥n  **Dominio:** Inteligencia Artificial Generativa en Educaci√≥n  



---**Cadena de b√∫squeda:** "generative artificial intelligence"**Cadena de b√∫squeda:** "generative artificial intelligence"



## üìä ESTADO ACTUAL DEL PROYECTO (23 de Enero, 2025)



### ‚úÖ COMPLETADO (40%)------



#### **‚úÖ Requerimiento 2: Algoritmos de Similitud Textual** (100%)

- ‚úÖ 6 algoritmos implementados: Levenshtein, TF-IDF + Coseno, Jaccard, N-gramas, BERT, Sentence-BERT

- ‚úÖ 8 endpoints REST funcionales con documentaci√≥n OpenAPI## üéØ OBJETIVO GENERAL## üéØ OBJETIVO GENERAL

- ‚úÖ Suite de tests completa (26 tests pasando)

- ‚úÖ Documentaci√≥n matem√°tica paso a paso para cada algoritmo

- ‚úÖ UI capaz de comparar abstracts de publicaciones

Implementar una plataforma web automatizada para an√°lisis bibliom√©trico avanzado de publicaciones cient√≠ficas sobre IA Generativa, integrando algoritmos cl√°sicos y modernos de ML/NLP, con capacidades de visualizaci√≥n interactiva y exportaci√≥n de resultados.Implementar una plataforma web automatizada para an√°lisis bibliom√©trico avanzado de publicaciones cient√≠ficas sobre IA Generativa, integrando algoritmos cl√°sicos y modernos de ML/NLP, con capacidades de visualizaci√≥n interactiva y exportaci√≥n de resultados.

**Archivos:**

- `app/services/ml_analysis/similarity/` - 7 m√≥dulos (1,800 l√≠neas)

- `app/api/v1/similarity.py` - 742 l√≠neas

- `tests/test_similarity_*.py` - 26 tests------



#### **‚úÖ Parsers Bibliogr√°ficos** (100%)

- ‚úÖ BibTeX Parser con manejo robusto de braces anidados

- ‚úÖ RIS Parser con soporte completo de tags## üìä ESTADO ACTUAL DEL PROYECTO## üìä ESTADO ACTUAL DEL PROYECTO

- ‚úÖ CSV Parser con normalizaci√≥n flexible

- ‚úÖ Unificador autom√°tico con detecci√≥n de formatos

- ‚úÖ Suite de tests completa (7 tests pasando)

### ‚úÖ **Completado (33%)**### ‚úÖ Infraestructura Completada

**Archivos:**

- `app/services/data_acquisition/parsers/` - 4 m√≥dulos (1,380 l√≠neas)- [x] Estructura de directorios Backend y Frontend

- `test_parsers.py` - 7 tests

#### **Requerimiento 2: Algoritmos de Similitud Textual** ‚úÖ- [x] FastAPI configurado con CORS y middlewares

#### **‚öôÔ∏è Infraestructura Base** (100%)

- ‚úÖ FastAPI con CORS y middlewares- [x] 6 algoritmos implementados (Levenshtein, TF-IDF, Jaccard, N-gramas, BERT, Sentence-BERT)- [x] Dependencias Python instaladas (requirements.txt)

- ‚úÖ Estructura modular Backend + Frontend

- ‚úÖ Sistema de logging y manejo de errores- [x] 8 endpoints REST funcionales- [x] Aplicaci√≥n React con TypeScript y Vite

- ‚úÖ Dependencias instaladas (requirements.txt con 79 paquetes)

- ‚úÖ .gitignore configurado- [x] Suite de tests completa (32/32 pasando)- [x] Dependencias frontend (MUI, D3, Plotly, Recharts)

- ‚úÖ Tests automatizados (pytest)

### ‚úÖ **Completado (45%)**

#### **Requerimiento 2: Algoritmos de Similitud Textual** ‚úÖ

- [x] 6 algoritmos implementados (Levenshtein, TF-IDF, Jaccard, N-gramas, BERT, Sentence-BERT)
- [x] 8 endpoints REST funcionales
- [x] Suite de tests completa (39/39 pasando)
- [x] Documentaci√≥n matem√°tica integrada
- [x] Sistema de logging y manejo de errores

- **Archivos:** 7 m√≥dulos de similitud + 3 archivos de tests (2,500 l√≠neas)

---

#### **Requerimiento 1: Automatizaci√≥n de Descarga** ‚úÖ

**Implementado:**
- ‚úÖ **ACM Digital Library Scraper** - Web scraping con BeautifulSoup (450 l√≠neas)
- ‚úÖ **SAGE Journals Scraper** - Multi-selector CSS (448 l√≠neas)
- ‚úÖ **ScienceDirect Scraper** - API de Elsevier con mock data (580 l√≠neas)
- ‚úÖ **CrossRefScraper** - API de CrossRef totalmente funcional
- ‚úÖ **Sistema de deduplicaci√≥n** - DOI, hash MD5, fuzzy matching
- ‚úÖ **UnifiedDownloader** - Coordinador de m√∫ltiples fuentes
- ‚úÖ **Suite completa de tests** - 17/18 tests pasando (1 skipped)
- ‚úÖ **8 Endpoints API de descarga** - `/api/v1/data/*`

**Archivos:**
- `app/services/data_acquisition/*.py` - 2,400 l√≠neas
- `test_data_acquisition.py` - 450 l√≠neas (18 tests)

---

#### **Requerimiento 6: Parsers Bibliogr√°ficos** ‚úÖ

- [x] BibTeXParser con manejo robusto de braces anidados
- [x] RISParser con soporte completo de tags
- [x] CSVParser con normalizaci√≥n flexible
- [x] PublicationUnifier con auto-detecci√≥n de formatos
- [x] Suite de tests completa (7/7 pasando)

- **Archivos:** 4 parsers + 1 archivo de tests (1,380 l√≠neas)

---

### ‚è≥ Pendiente de Implementaci√≥n (55%)

- [ ] Requerimiento 3: An√°lisis de frecuencias de conceptos
- [ ] Requerimiento 4: Clustering jer√°rquico y dendrogramas
- [ ] Requerimiento 5: Visualizaciones interactivas
- [ ] Requerimiento 6: Despliegue y documentaci√≥n t√©cnica

---

## üóìÔ∏è CRONOGRAMA DE IMPLEMENTACI√ìN

### **Fase 1: Requerimiento 1 - Automatizaci√≥n de Descarga** ‚úÖ COMPLETADO

- ‚úÖ D√≠as 1-2: Scrapers (ACM, SAGE, ScienceDirect) y conectores API
- ‚úÖ D√≠a 3: Unificaci√≥n y eliminaci√≥n de duplicados
- ‚úÖ D√≠as 4-5: Testing y validaci√≥n (56/57 tests pasando)

- ‚ùå C√°lculo de precisi√≥n de palabras generadas

- ‚ùå Visualizaci√≥n de frecuencias- [ ] Endpoints REST



**Categor√≠a a analizar:**- **Prioridad:** ALTA | **Estimado:** 6 horas### **Fase 2: Requerimiento 2 - Similitud Textual** (7 d√≠as)

```

Concepts of Generative AI in Education:- D√≠as 6-7: Algoritmos cl√°sicos (Levenshtein, TF-IDF, Jaccard, N-gramas)

- Generative models, Prompting, Machine learning, Multimodality

- Fine-tuning, Training data, Algorithmic bias#### **Requerimiento 4: Clustering Jer√°rquico**- D√≠as 8-9: Algoritmos con IA (BERT, Sentence-BERT)

- Explainability, Transparency, Ethics, Privacy

- Personalization, Human-AI interaction, AI literacy, Co-creation- [ ] Algoritmo de clustering- D√≠as 10-11: Documentaci√≥n matem√°tica detallada

```

- [ ] Generaci√≥n de dendrogramas- D√≠a 12: UI para comparaci√≥n de abstracts

**Estimado:** 2 d√≠as (12 horas)

- [ ] Identificaci√≥n autom√°tica de grupos

#### **Requerimiento 4: Clustering Jer√°rquico** (0%)

- ‚ùå Preprocesamiento de abstracts- [ ] M√©tricas de evaluaci√≥n### **Fase 3: Requerimiento 3 - Frecuencias de Conceptos** (4 d√≠as)

- ‚ùå Implementaci√≥n de 3 algoritmos de clustering (Ward, Average Linkage, Complete Linkage)

- ‚ùå Generaci√≥n de dendrogramas interactivos- [ ] Endpoints REST- D√≠a 13: An√°lisis de frecuencias de categor√≠as predefinidas

- ‚ùå Evaluaci√≥n comparativa (Silhouette Score)

- **Prioridad:** ALTA | **Estimado:** 6 horas- D√≠a 14: Generaci√≥n autom√°tica de palabras asociadas con NLP

**Estimado:** 3 d√≠as (18 horas)

- D√≠a 15: M√©tricas de precisi√≥n

#### **Requerimiento 5: Visualizaciones Interactivas** (0%)

- ‚ùå Mapa de calor geogr√°fico (distribuci√≥n por pa√≠s del primer autor)#### **Requerimiento 5: Scrapers Adicionales**- D√≠a 16: Visualizaciones de frecuencias

- ‚ùå Nube de palabras din√°mica (t√©rminos frecuentes en abstracts y keywords)

- ‚ùå L√≠nea temporal de publicaciones por a√±o y revista- [ ] ACM Digital Library scraper

- ‚ùå Exportaci√≥n a PDF de visualizaciones

- [ ] SAGE Journals scraper### **Fase 4: Requerimiento 4 - Clustering Jer√°rquico** (5 d√≠as)

**Estimado:** 2 d√≠as (12 horas)

- [ ] ScienceDirect scraper- D√≠as 17-18: Preprocesamiento de texto y vectorizaci√≥n

#### **Requerimiento 6: Despliegue y Documentaci√≥n** (0%)

- ‚ùå Dockerizaci√≥n del proyecto- **Prioridad:** MEDIA | **Estimado:** 12 horas- D√≠a 19: Implementaci√≥n de 3 algoritmos de clustering

- ‚ùå Configuraci√≥n de CI/CD

- ‚ùå Documentaci√≥n t√©cnica completa- D√≠a 20: Generaci√≥n de dendrogramas interactivos con D3.js

- ‚ùå Despliegue en producci√≥n

---- D√≠a 21: Evaluaci√≥n comparativa (Silhouette Score)

**Estimado:** 2 d√≠as (12 horas)



---

## üßπ LIMPIEZA REALIZADA (23 de Enero, 2025)### **Fase 5: Requerimiento 5 - Visualizaciones** (4 d√≠as)

## üóìÔ∏è PLAN DE ACCI√ìN PRIORIZADO

- D√≠a 22: Mapa de calor geogr√°fico (Plotly/Leaflet)

### **Fase 1: Completar Requerimiento 1** (2 d√≠as)

**Prioridad: CR√çTICA** - Es la base para todos los dem√°s requerimientos### **Archivos Eliminados**- D√≠a 23: Nube de palabras din√°mica (D3.js)



1. **D√≠a 1: Completar Scrapers**- D√≠a 24: L√≠nea temporal por a√±o y revista (Recharts)

   - ‚úÖ Verificar y completar ACM Scraper

   - ‚úÖ Verificar y completar SAGE Scraper#### **Debug y Temporales**- D√≠a 25: Exportaci√≥n a PDF (jsPDF + html2canvas)

   - üÜï Implementar ScienceDirect Scraper

   - ‚úÖ Testing de scrapers individuales- ‚ùå `Backend/debug_bibtex.py` - Script de debug temporal



2. **D√≠a 2: Integraci√≥n y API**- ‚ùå `Backend/test_debug.py` - Test de debug### **Fase 6: Requerimiento 6 - Despliegue y Documentaci√≥n** (4 d√≠as)

   - üÜï Crear endpoints API `/api/v1/data`

   - ‚úÖ Integrar UnifiedDownloader con todos los scrapers- ‚ùå `Backend/test_requerimiento1.log` - Log temporal- D√≠a 26: Documentaci√≥n t√©cnica de arquitectura

   - ‚úÖ Implementar sistema de jobs en background (Celery/BackgroundTasks)

   - ‚úÖ Testing de integraci√≥n completa- ‚ùå `SESION_RESUMEN.md` - Documentaci√≥n temporal- D√≠a 27: Documentaci√≥n de algoritmos implementados

   - üìù Generar dataset unificado de prueba

- ‚ùå `SESION_PARSERS.md` - Documentaci√≥n temporal- D√≠a 28: Dockerizaci√≥n y CI/CD

**Entregables:**

- Sistema autom√°tico de descarga funcionando- ‚ùå `PROGRESO.md` - Duplicado (se mantiene PROGRESO_ACTUALIZADO.md)- D√≠a 29: Despliegue en producci√≥n

- API REST para descarga (`/download`, `/status/{job_id}`, `/unified`, `/duplicates`)

- Dataset unificado en `data/downloads/unified.json`

- Reporte de duplicados en `data/downloads/duplicates_report.json`

#### **Cache y Builds**### **Fase 7: Testing Final y Optimizaci√≥n** (2 d√≠as)

---

- ‚ùå `Backend/__pycache__/` (recursivo)- D√≠a 30: Testing integral y correcciones

### **Fase 2: Requerimiento 3 - An√°lisis de Frecuencias** (1.5 d√≠as)

- ‚ùå `Backend/.pytest_cache/`- D√≠a 31: Optimizaci√≥n de rendimiento

1. **Paso 1: An√°lisis de Categor√≠a Predefinida** (0.5 d√≠a)

   - Implementar `FrequencyAnalyzer` en `app/services/analytics/`- ‚ùå Todos los `__pycache__/` en subdirectorios

   - Buscar palabras asociadas en abstracts

   - Calcular frecuencias y generar estad√≠sticas**TOTAL ESTIMADO: 31 d√≠as (‚âà 6-7 semanas)**



2. **Paso 2: Generaci√≥n Autom√°tica de Palabras** (0.5 d√≠a)#### **Datos Temporales**

   - Usar NLP (NLTK/spaCy) para extracci√≥n de t√©rminos

   - Lematizaci√≥n y eliminaci√≥n de stopwords- ‚ùå `Backend/data/downloads/*.json` - Descargas de prueba---

   - Generar top 15 palabras asociadas autom√°ticamente

- ‚ùå `Backend/data/downloads/*.bib`

3. **Paso 3: M√©tricas de Precisi√≥n** (0.5 d√≠a)

   - Calcular precisi√≥n comparando palabras generadas vs predefinidas- ‚ùå `Backend/data/downloads/*.ris`## üèóÔ∏è ARQUITECTURA DEL SISTEMA

   - Implementar endpoints API

   - Testing y visualizaci√≥n b√°sica- ‚ùå `Backend/data/downloads/*.csv`



**Entregables:**```

- `app/services/analytics/frequency_analyzer.py`

- Endpoints: `/api/v1/analytics/frequencies`, `/api/v1/analytics/associated-words`**Total eliminado:** ~35 archivos + directorios de cache‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

- Reporte de precisi√≥n

‚îÇ                      FRONTEND (React + TS)                   ‚îÇ

---

### **Nuevo .gitignore Creado**‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ

### **Fase 3: Requerimiento 4 - Clustering Jer√°rquico** (2 d√≠as)

‚îÇ  ‚îÇ   D3.js     ‚îÇ  ‚îÇ  Plotly.js  ‚îÇ  ‚îÇ  Recharts   ‚îÇ         ‚îÇ

1. **Paso 1: Preprocesamiento** (0.5 d√≠a)

   - Pipeline de limpieza de textoExcluye:‚îÇ  ‚îÇ Dendrogramas‚îÇ  ‚îÇ Mapas Calor ‚îÇ  ‚îÇ  L√≠neas     ‚îÇ         ‚îÇ

   - Vectorizaci√≥n (TF-IDF o embeddings)

   - Normalizaci√≥n- `__pycache__/`, `*.pyc`, `*.pyo`‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ



2. **Paso 2: Algoritmos de Clustering** (1 d√≠a)- `venv/`, `.venv/`, `env/`‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   - Implementar Ward Linkage

   - Implementar Average Linkage- `.pytest_cache/`, `.coverage`                         ‚îÇ HTTP/REST API

   - Implementar Complete Linkage

   - M√©tricas de evaluaci√≥n (Silhouette Score)- `.env`, `.env.local`‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê



3. **Paso 3: Dendrogramas** (0.5 d√≠a)- `Backend/data/downloads/*` (excepto `.gitkeep`)‚îÇ                   BACKEND (FastAPI)                          ‚îÇ

   - Generar dendrogramas con scipy/matplotlib

   - Implementar endpoints API- `Backend/logs/*.log`‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ

   - Testing

- `Backend/models/*.pkl`, `*.h5`, `*.pt`‚îÇ  ‚îÇ            API ENDPOINTS (v1)                        ‚îÇ   ‚îÇ

**Entregables:**

- `app/services/ml_analysis/clustering/hierarchical_clustering.py`- `SESION_*.md` (archivos de sesi√≥n temporal)‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ

- Endpoints: `/api/v1/clustering/hierarchical`

- Dendrogramas en formato JSON para visualizaci√≥n- `debug_*.py`, `test_debug.py`‚îÇ  ‚îÇ /api/v1/data          - Data Acquisition Service    ‚îÇ   ‚îÇ



---- `node_modules/`, `dist/`, `build/`‚îÇ  ‚îÇ /api/v1/ml            - ML & NLP Service             ‚îÇ   ‚îÇ



### **Fase 4: Requerimiento 5 - Visualizaciones** (2 d√≠as)- `.DS_Store`, `Thumbs.db`‚îÇ  ‚îÇ /api/v1/analytics     - Analytics Service            ‚îÇ   ‚îÇ



1. **Backend: Endpoints de Datos** (0.5 d√≠a)‚îÇ  ‚îÇ /api/v1/viz           - Visualization Service        ‚îÇ   ‚îÇ

   - `/api/v1/viz/geographic-distribution`

   - `/api/v1/viz/word-cloud-data`---‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ

   - `/api/v1/viz/timeline`

‚îÇ                                                              ‚îÇ

2. **Frontend: Implementaci√≥n de Visualizaciones** (1 d√≠a)

   - Mapa de calor geogr√°fico (Plotly/Leaflet)## üèóÔ∏è ESTRUCTURA FINAL DEL REPOSITORIO‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ

   - Nube de palabras din√°mica (D3.js/react-wordcloud)

   - L√≠nea temporal (Recharts)‚îÇ  ‚îÇ         SERVICIOS DE NEGOCIO                        ‚îÇ    ‚îÇ



3. **Exportaci√≥n PDF** (0.5 d√≠a)```‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ

   - Implementar jsPDF + html2canvas

   - Endpoint `/api/v1/viz/export-pdf`ProyectoAnalisisAlgoritmos/‚îÇ  ‚îÇ üîç DataAcquisitionService                           ‚îÇ    ‚îÇ



**Entregables:**‚îú‚îÄ‚îÄ .gitignore                      # Control de versiones‚îÇ  ‚îÇ    - Scrapers (ACM, SAGE, ScienceDirect)           ‚îÇ    ‚îÇ

- Componentes React en `Frontend/bibliometric-app/src/components/visualizations/`

- Sistema de exportaci√≥n PDF funcional‚îú‚îÄ‚îÄ README.md                       # Documentaci√≥n principal‚îÇ  ‚îÇ    - Unificaci√≥n y deduplicaci√≥n                    ‚îÇ    ‚îÇ



---‚îú‚îÄ‚îÄ PLAN_IMPLEMENTACION.md         # Este archivo‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ



### **Fase 5: Requerimiento 6 - Despliegue** (1.5 d√≠as)‚îú‚îÄ‚îÄ PROGRESO_ACTUALIZADO.md        # Estado detallado del proyecto‚îÇ  ‚îÇ ü§ñ MLAnalysisService                                ‚îÇ    ‚îÇ



1. **Dockerizaci√≥n** (0.5 d√≠a)‚îÇ‚îÇ  ‚îÇ    - Similitud cl√°sica (Levenshtein, TF-IDF, etc.) ‚îÇ    ‚îÇ

   - Dockerfile para Backend

   - Dockerfile para Frontend‚îú‚îÄ‚îÄ Backend/‚îÇ  ‚îÇ    - Similitud IA (BERT, Sentence-BERT)            ‚îÇ    ‚îÇ

   - docker-compose.yml

‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Aplicaci√≥n FastAPI principal‚îÇ  ‚îÇ    - Clustering jer√°rquico (Ward, Average, etc.)   ‚îÇ    ‚îÇ

2. **Documentaci√≥n** (0.5 d√≠a)

   - README.md completo‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # Dependencias Python‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ

   - Documentaci√≥n t√©cnica de algoritmos

   - Gu√≠a de instalaci√≥n y uso‚îÇ   ‚îú‚îÄ‚îÄ pytest.ini                 # Configuraci√≥n de pytest‚îÇ  ‚îÇ üìä AnalyticsService                                 ‚îÇ    ‚îÇ



3. **Despliegue** (0.5 d√≠a)‚îÇ   ‚îú‚îÄ‚îÄ .env.example              # Template de variables de entorno‚îÇ  ‚îÇ    - Frecuencias de conceptos                       ‚îÇ    ‚îÇ

   - Configurar CI/CD (GitHub Actions)

   - Deploy en servidor (Heroku/Railway/DigitalOcean)‚îÇ   ‚îÇ‚îÇ  ‚îÇ    - M√©tricas bibliom√©tricas                        ‚îÇ    ‚îÇ

   - Testing en producci√≥n

‚îÇ   ‚îú‚îÄ‚îÄ app/‚îÇ  ‚îÇ    - Generaci√≥n de palabras asociadas               ‚îÇ    ‚îÇ

**Entregables:**

- Aplicaci√≥n desplegada y accesible‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ

- Documentaci√≥n completa

- Repositorio limpio y profesional‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/‚îÇ  ‚îÇ üìà VisualizationService                             ‚îÇ    ‚îÇ



---‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ  ‚îÇ    - Mapas de calor geogr√°ficos                     ‚îÇ    ‚îÇ



## üèóÔ∏è ARQUITECTURA DEL SISTEMA‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/‚îÇ  ‚îÇ    - Nubes de palabras                              ‚îÇ    ‚îÇ



```‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py‚îÇ  ‚îÇ    - L√≠neas temporales                              ‚îÇ    ‚îÇ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

‚îÇ                  FRONTEND (React + TypeScript)               ‚îÇ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ data_acquisition.py‚îÇ  ‚îÇ    - Exportaci√≥n PDF                                ‚îÇ    ‚îÇ

‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ

‚îÇ  ‚îÇ   D3.js      ‚îÇ  ‚îÇ  Plotly.js   ‚îÇ  ‚îÇ  Recharts    ‚îÇ      ‚îÇ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ similarity.py‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ

‚îÇ  ‚îÇ Dendrogramas ‚îÇ  ‚îÇ Mapa Calor   ‚îÇ  ‚îÇ  Timeline    ‚îÇ      ‚îÇ

‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ‚îÇ   ‚îÇ   ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                            ‚îÇ HTTP/REST API‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/                         ‚îÇ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

‚îÇ                   BACKEND (FastAPI)                          ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ

‚îÇ  ‚îÇ         API ENDPOINTS (/api/v1)                        ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py‚îÇ               CAPA DE DATOS                                  ‚îÇ

‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ

‚îÇ  ‚îÇ /data/*          - Descarga y unificaci√≥n (Req 1)     ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ

‚îÇ  ‚îÇ /similarity/*    - Similitud textual (Req 2) ‚úÖ       ‚îÇ ‚îÇ

‚îÇ  ‚îÇ /analytics/*     - Frecuencias (Req 3)                ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ    Redis     ‚îÇ  ‚îÇ  File System ‚îÇ      ‚îÇ

‚îÇ  ‚îÇ /clustering/*    - Clustering jer√°rquico (Req 4)      ‚îÇ ‚îÇ

‚îÇ  ‚îÇ /viz/*           - Visualizaciones (Req 5)            ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ  ‚îÇ  Metadatos   ‚îÇ  ‚îÇ    Cache     ‚îÇ  ‚îÇ  Archivos    ‚îÇ      ‚îÇ

‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ

‚îÇ                                                              ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ publication.py‚îÇ  ‚îÇ  Publicac.   ‚îÇ  ‚îÇ  Embeddings  ‚îÇ  ‚îÇ  CSV/BibTeX  ‚îÇ      ‚îÇ

‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ

‚îÇ  ‚îÇ              SERVICIOS DE NEGOCIO                      ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ

‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ

‚îÇ  ‚îÇ üì• DataAcquisitionService (Req 1) - 40% ‚è≥            ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îÇ  ‚îÇ    - Scrapers: ACM, SAGE, ScienceDirect              ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - Parsers: BibTeX, RIS, CSV ‚úÖ                     ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py```

‚îÇ  ‚îÇ    - Deduplicador inteligente                         ‚îÇ ‚îÇ

‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics/               # Placeholder (Req. 3)

‚îÇ  ‚îÇ ü§ñ MLAnalysisService                                  ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - Similitud (Req 2) ‚úÖ                             ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py---

‚îÇ  ‚îÇ    - Clustering (Req 4) - 0% ‚ùå                       ‚îÇ ‚îÇ

‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_acquisition/

‚îÇ  ‚îÇ üìä AnalyticsService (Req 3) - 0% ‚ùå                   ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - Frecuencias de conceptos                         ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py## üìù REQUERIMIENTO 1: AUTOMATIZACI√ìN DE DESCARGA DE DATOS

‚îÇ  ‚îÇ    - Generaci√≥n de palabras asociadas                 ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - M√©tricas de precisi√≥n                            ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py

‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ

‚îÇ  ‚îÇ üìà VisualizationService (Req 5) - 0% ‚ùå               ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crossref_scraper.py### **Objetivo**

‚îÇ  ‚îÇ    - Mapas de calor geogr√°ficos                       ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - Nubes de palabras din√°micas                      ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deduplicator.pyAutomatizar la descarga de publicaciones cient√≠ficas desde ACM, SAGE y ScienceDirect, unificar los datos en un archivo √∫nico sin duplicados, y generar un reporte de productos eliminados.

‚îÇ  ‚îÇ    - L√≠neas temporales                                ‚îÇ ‚îÇ

‚îÇ  ‚îÇ    - Exportaci√≥n PDF                                  ‚îÇ ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unified_downloader.py

‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ

‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parsers/### **Componentes a Implementar**

                            ‚îÇ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py

‚îÇ                   CAPA DE DATOS                              ‚îÇ

‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bibtex_parser.py   # ‚úÖ 450 l√≠neas#### 1.1. **Scrapers y Conectores API**

‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ    Redis     ‚îÇ  ‚îÇ  File System ‚îÇ      ‚îÇ

‚îÇ  ‚îÇ  (Futuro)    ‚îÇ  ‚îÇ    Cache     ‚îÇ  ‚îÇ  JSON/CSV    ‚îÇ      ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ris_parser.py      # ‚úÖ 420 l√≠neas

‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ

‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ csv_parser.py      # ‚úÖ 290 l√≠neas**Archivos a crear:**

```

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ unifier.py         # ‚úÖ 220 l√≠neas```

---

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_analysis/Backend/app/services/data_acquisition/

## üì¶ ESTRUCTURA DE ARCHIVOS

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îú‚îÄ‚îÄ __init__.py

```

ProyectoAnalisisAlgoritmos/‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ similarity/‚îú‚îÄ‚îÄ base_scraper.py          # Clase base abstracta

‚îú‚îÄ‚îÄ .gitignore                          ‚úÖ

‚îú‚îÄ‚îÄ README.md                           ‚úÖ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py‚îú‚îÄ‚îÄ acm_scraper.py           # Scraper para ACM Digital Library

‚îú‚îÄ‚îÄ PLAN_IMPLEMENTACION.md             ‚úÖ (este archivo)

‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base_similarity.py       # ‚úÖ‚îú‚îÄ‚îÄ sage_scraper.py          # Scraper para SAGE Publications

‚îú‚îÄ‚îÄ Backend/

‚îÇ   ‚îú‚îÄ‚îÄ main.py                        ‚úÖ FastAPI app (247 l√≠neas)‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ levenshtein.py          # ‚úÖ‚îú‚îÄ‚îÄ sciencedirect_scraper.py # Scraper para ScienceDirect

‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt               ‚úÖ 79 dependencias

‚îÇ   ‚îú‚îÄ‚îÄ pytest.ini                     ‚úÖ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ tfidf_cosine.py         # ‚úÖ‚îú‚îÄ‚îÄ unified_downloader.py    # Orquestador de descargas

‚îÇ   ‚îÇ

‚îÇ   ‚îú‚îÄ‚îÄ app/‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ jaccard.py              # ‚úÖ‚îî‚îÄ‚îÄ parsers/

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               ‚úÖ

‚îÇ   ‚îÇ   ‚îÇ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ngrams.py               # ‚úÖ    ‚îú‚îÄ‚îÄ __init__.py

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/v1/

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           ‚úÖ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bert_embeddings.py      # ‚úÖ    ‚îú‚îÄ‚îÄ bibtex_parser.py     # Parser para formato BibTex

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ similarity.py         ‚úÖ (742 l√≠neas) - Req 2

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_acquisition.py   ‚è≥ (pendiente completar) - Req 1‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ sentence_bert.py        # ‚úÖ    ‚îú‚îÄ‚îÄ ris_parser.py        # Parser para formato RIS

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py          ‚ùå (crear) - Req 3

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clustering.py         ‚ùå (crear) - Req 4‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization/           # Placeholder    ‚îî‚îÄ‚îÄ csv_parser.py        # Parser para formato CSV

‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py      ‚ùå (crear) - Req 5

‚îÇ   ‚îÇ   ‚îÇ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py```

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           ‚úÖ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/

‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ publication.py        ‚úÖ

‚îÇ   ‚îÇ   ‚îÇ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py**Funcionalidades clave:**

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_acquisition/‚îÇ   ‚îÇ

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py           ‚úÖ (368 l√≠neas)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crossref_scraper.py       ‚è≥ (verificar)‚îÇ   ‚îú‚îÄ‚îÄ data/```python

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acm_scraper.py            ‚è≥ (completar)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sage_scraper.py           ‚è≥ (completar)‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ downloads/# base_scraper.py - Clase base abstracta

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sciencedirect_scraper.py  ‚ùå (crear)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deduplicator.py           ‚è≥ (verificar)‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ .gitkeepfrom abc import ABC, abstractmethod

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unified_downloader.py     ‚úÖ (431 l√≠neas)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parsers/‚îÇ   ‚îÇfrom typing import List, Dict, Any

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bibtex_parser.py      ‚úÖ (450 l√≠neas)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ris_parser.py         ‚úÖ (420 l√≠neas)‚îÇ   ‚îú‚îÄ‚îÄ logs/

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ csv_parser.py         ‚úÖ (290 l√≠neas)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ unifier.py            ‚úÖ (220 l√≠neas)‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeepclass BaseScraper(ABC):

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_analysis/‚îÇ   ‚îÇ    """Clase base para todos los scrapers de bases de datos cient√≠ficas."""

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ similarity/                    ‚úÖ Req 2

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ levenshtein.py            ‚úÖ‚îÇ   ‚îú‚îÄ‚îÄ models/    

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tfidf_cosine.py           ‚úÖ

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jaccard.py                ‚úÖ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep    @abstractmethod

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ngrams.py                 ‚úÖ

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bert_embeddings.py        ‚úÖ‚îÇ   ‚îÇ    async def search(self, query: str, max_results: int) -> List[Dict[str, Any]]:

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sentence_bert.py          ‚úÖ

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ‚îÇ   ‚îî‚îÄ‚îÄ tests/        """Busca publicaciones con la query especificada."""

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clustering/                    ‚ùå Req 4

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py               ‚ùå (crear)‚îÇ       ‚îú‚îÄ‚îÄ __init__.py        pass

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ hierarchical.py           ‚ùå (crear)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dendrogram_generator.py   ‚ùå (crear)‚îÇ       ‚îú‚îÄ‚îÄ test_similarity_endpoints.py   # ‚úÖ 23 tests    

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ

‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics/                         ‚ùå Req 3‚îÇ       ‚îú‚îÄ‚îÄ test_similarity_api.py         # ‚úÖ 3 tests    @abstractmethod

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                   ‚úÖ (placeholder)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frequency_analyzer.py         ‚ùå (crear)‚îÇ       ‚îú‚îÄ‚îÄ test_parsers.py                # ‚úÖ 7 tests    async def download_metadata(self, publication_id: str) -> Dict[str, Any]:

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ word_extractor.py             ‚ùå (crear)

‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ‚îÇ       ‚îú‚îÄ‚îÄ test_similitud.py              # ‚úÖ 4 tests        """Descarga metadatos completos de una publicaci√≥n."""

‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization/                     ‚ùå Req 5

‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py                   ‚úÖ (placeholder)‚îÇ       ‚îî‚îÄ‚îÄ test_requerimiento1.py         # ‚úÖ 2 tests        pass

‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ geographic_map.py             ‚ùå (crear)

‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pdf_exporter.py               ‚ùå (crear)‚îÇ    

‚îÇ   ‚îÇ   ‚îÇ

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/‚îî‚îÄ‚îÄ Frontend/    @abstractmethod

‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py                       ‚úÖ

‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ bibliometric-app/    def export_to_format(self, publications: List[Dict], format: str) -> str:

‚îÇ   ‚îú‚îÄ‚îÄ data/downloads/                           ‚úÖ (.gitkeep)

‚îÇ   ‚îú‚îÄ‚îÄ logs/                                     ‚úÖ (.gitkeep)        ‚îú‚îÄ‚îÄ package.json        """Exporta publicaciones al formato especificado (bibtex, ris, csv)."""

‚îÇ   ‚îú‚îÄ‚îÄ models/                                   ‚úÖ (.gitkeep)

‚îÇ   ‚îÇ        ‚îú‚îÄ‚îÄ vite.config.ts        pass

‚îÇ   ‚îî‚îÄ‚îÄ tests/

‚îÇ       ‚îú‚îÄ‚îÄ test_parsers.py                      ‚úÖ (7 tests pasando)        ‚îú‚îÄ‚îÄ tsconfig.json```

‚îÇ       ‚îú‚îÄ‚îÄ test_similitud.py                    ‚úÖ (4 tests pasando)

‚îÇ       ‚îú‚îÄ‚îÄ test_similarity_api.py               ‚úÖ (3 tests pasando)        ‚îú‚îÄ‚îÄ .gitignore

‚îÇ       ‚îú‚îÄ‚îÄ test_similarity_endpoints.py         ‚úÖ (23 tests pasando)

‚îÇ       ‚îî‚îÄ‚îÄ test_requerimiento1.py               ‚úÖ (2 tests pasando)        ‚îú‚îÄ‚îÄ public/**Tecnolog√≠as:**

‚îÇ

‚îî‚îÄ‚îÄ Frontend/bibliometric-app/        ‚îî‚îÄ‚îÄ src/- `scrapy` para scraping web

    ‚îú‚îÄ‚îÄ package.json                             ‚úÖ

    ‚îú‚îÄ‚îÄ vite.config.ts                           ‚úÖ            ‚îú‚îÄ‚îÄ main.tsx- `aiohttp` para peticiones as√≠ncronas

    ‚îî‚îÄ‚îÄ src/

        ‚îú‚îÄ‚îÄ main.tsx                             ‚úÖ            ‚îú‚îÄ‚îÄ App.tsx- `crossref-commons` para API de CrossRef

        ‚îú‚îÄ‚îÄ App.tsx                              ‚úÖ

        ‚îú‚îÄ‚îÄ components/                          ‚è≥ (pendiente)            ‚îú‚îÄ‚îÄ types/- `scholarly` para b√∫squedas acad√©micas

        ‚îî‚îÄ‚îÄ services/api.ts                      ‚úÖ

```            ‚îú‚îÄ‚îÄ services/- `habanero` para API de CrossRef



---            ‚îî‚îÄ‚îÄ hooks/- `beautifulsoup4` para parsing HTML



## ‚úÖ CHECKLIST DE FINALIZACI√ìN```



### Requerimiento 1: Automatizaci√≥n de Descarga#### 1.2. **Sistema de Unificaci√≥n y Deduplicaci√≥n**

- [x] Parsers BibTeX, RIS, CSV ‚úÖ

- [x] BaseScraper y estructura ‚úÖ**Estad√≠sticas:**

- [ ] ACM Scraper funcional

- [ ] SAGE Scraper funcional- **Archivos Python activos:** 35**Archivos a crear:**

- [ ] ScienceDirect Scraper funcional

- [ ] Deduplicador probado- **L√≠neas de c√≥digo:** ~4,500```

- [ ] UnifiedDownloader integrado

- [ ] Endpoints API `/api/v1/data/*`- **Tests:** 39 totales (11 pasando)Backend/app/services/data_acquisition/

- [ ] Testing completo

- [ ] Dataset unificado generado- **Cobertura:** ~70%‚îú‚îÄ‚îÄ deduplicator.py          # Sistema de eliminaci√≥n de duplicados

- [ ] Reporte de duplicados generado

‚îú‚îÄ‚îÄ unifier.py               # Unificaci√≥n de formatos

### Requerimiento 2: Similitud Textual

- [x] 6 algoritmos implementados ‚úÖ---‚îî‚îÄ‚îÄ models/

- [x] Documentaci√≥n matem√°tica ‚úÖ

- [x] Endpoints API ‚úÖ    ‚îú‚îÄ‚îÄ publication.py       # Modelo de datos unificado

- [x] Testing completo ‚úÖ

- [x] UI para comparaci√≥n ‚úÖ## üì¶ DEPENDENCIAS    ‚îî‚îÄ‚îÄ duplicate_report.py  # Modelo de reporte de duplicados



### Requerimiento 3: Frecuencias```

- [ ] FrequencyAnalyzer

- [ ] An√°lisis de categor√≠a predefinida### **Backend (Python 3.13)**

- [ ] Generaci√≥n de palabras con NLP

- [ ] M√©tricas de precisi√≥n**Algoritmo de deduplicaci√≥n:**

- [ ] Endpoints API

- [ ] Visualizaciones```txt



### Requerimiento 4: Clustering# Web Framework```python

- [ ] 3 algoritmos de clustering

- [ ] Preprocesamiento de textofastapi==0.119.1# deduplicator.py

- [ ] Generaci√≥n de dendrogramas

- [ ] Evaluaci√≥n (Silhouette Score)uvicorn==0.34.0import hashlib

- [ ] Endpoints API

- [ ] Testingpydantic==2.10.3from typing import List, Tuple, Dict



### Requerimiento 5: Visualizacionesfrom difflib import SequenceMatcher

- [ ] Mapa de calor geogr√°fico

- [ ] Nube de palabras din√°mica# ML/NLP

- [ ] L√≠nea temporal

- [ ] Exportaci√≥n PDFscikit-learn==1.6.1class Deduplicator:

- [ ] Endpoints API

- [ ] Integraci√≥n frontendsentence-transformers==3.4.0    """Sistema inteligente de eliminaci√≥n de duplicados."""



### Requerimiento 6: Desplieguepython-Levenshtein==0.27.0    

- [ ] Dockerfile Backend

- [ ] Dockerfile Frontend    def __init__(self, similarity_threshold: float = 0.95):

- [ ] docker-compose.yml

- [ ] CI/CD configurado# Testing        self.similarity_threshold = similarity_threshold

- [ ] Documentaci√≥n completa

- [ ] Aplicaci√≥n desplegadapytest==8.4.2        self.duplicates_report = []



---pytest-asyncio==0.25.2    



## üìà PROGRESO GENERALhttpx==0.28.1    def calculate_title_similarity(self, title1: str, title2: str) -> float:



``````        """Calcula similitud entre t√≠tulos usando SequenceMatcher."""

Requerimiento 1: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 40%

Requerimiento 2: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% ‚úÖ        return SequenceMatcher(None, title1.lower(), title2.lower()).ratio()

Requerimiento 3: [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   0%

Requerimiento 4: [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   0%### **Frontend (Node.js)**    

Requerimiento 5: [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   0%

Requerimiento 6: [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   0%    def generate_hash(self, publication: Dict) -> str:



TOTAL PROYECTO: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 40%```json        """Genera hash √∫nico basado en t√≠tulo y DOI."""

```

{        identifier = f"{publication.get('title', '')}_{publication.get('doi', '')}"

**Estimado para completar:** 9 d√≠as (54 horas de trabajo)

  "dependencies": {        return hashlib.md5(identifier.encode()).hexdigest()

---

    "react": "^18.3.1",    

## üöÄ PR√ìXIMOS PASOS INMEDIATOS

    "@mui/material": "^6.3.0",    def deduplicate(

1. **Verificar estado de scrapers existentes**

   - Revisar `acm_scraper.py`, `sage_scraper.py`, `crossref_scraper.py`    "recharts": "^2.15.0"        self, 

   - Identificar qu√© falta completar

  }        publications: List[Dict]

2. **Completar Requerimiento 1**

   - Implementar scrapers faltantes}    ) -> Tuple[List[Dict], List[Dict]]:

   - Crear endpoints API

   - Testing completo```        """

   - Generar dataset de prueba

        Elimina duplicados y retorna:

3. **Continuar con Requerimiento 3**

   - Implementar an√°lisis de frecuencias---        - Lista de publicaciones √∫nicas

   - NLP para generaci√≥n de palabras

        - Lista de duplicados eliminados

---

## üéØ PR√ìXIMOS PASOS        """

**√öltima actualizaci√≥n:** 23 de Enero, 2025  

**Versi√≥n:** 3.0          unique_publications = []

**Estado:** üü¢ En desarrollo activo (40% completado)

### **Inmediato**        duplicates = []

1. **Requerimiento 3:** An√°lisis de Frecuencias (6h)        seen_hashes = set()

2. **Requerimiento 4:** Clustering Jer√°rquico (6h)        

3. **Frontend:** Integraci√≥n b√°sica (8h)        for pub in publications:

            pub_hash = self.generate_hash(pub)

### **Corto Plazo**            

4. **Requerimiento 5:** Scrapers adicionales (12h)            # Verificar duplicados exactos por hash

5. **Visualizaciones:** Implementaci√≥n completa (10h)            if pub_hash in seen_hashes:

                duplicates.append(pub)

### **Largo Plazo**                continue

6. **Despliegue:** Producci√≥n (4h)            

7. **Documentaci√≥n:** Completa (8h)            # Verificar duplicados por similitud de t√≠tulo

            is_duplicate = False

---            for unique_pub in unique_publications:

                similarity = self.calculate_title_similarity(

## ‚úÖ CHECKLIST DE CALIDAD                    pub.get('title', ''),

                    unique_pub.get('title', '')

- [x] C√≥digo limpio y documentado                )

- [x] Tests pasando                if similarity >= self.similarity_threshold:

- [x] .gitignore configurado                    duplicates.append(pub)

- [x] Estructura modular                    is_duplicate = True

- [x] Logging implementado                    break

- [ ] Cobertura >80%            

- [ ] Frontend funcional            if not is_duplicate:

- [ ] CI/CD configurado                unique_publications.append(pub)

                seen_hashes.add(pub_hash)

---        

        return unique_publications, duplicates

**√öltima actualizaci√≥n:** 23 de Enero, 2025  ```

**Versi√≥n:** 2.0  

**Estado:** üü¢ En desarrollo activo#### 1.3. **Endpoints API**


**Archivos a crear:**
```
Backend/app/api/v1/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ data_acquisition.py  # Endpoints para descarga y unificaci√≥n
```

**Endpoints a implementar:**

```python
# data_acquisition.py
from fastapi import APIRouter, BackgroundTasks, HTTPException
from typing import List, Optional
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/data", tags=["Data Acquisition"])

class DownloadRequest(BaseModel):
    query: str
    sources: List[str]  # ['acm', 'sage', 'sciencedirect']
    max_results_per_source: int = 100
    export_format: str = 'json'  # json, bibtex, ris, csv

class DownloadResponse(BaseModel):
    job_id: str
    status: str
    message: str

@router.post("/download", response_model=DownloadResponse)
async def start_download(
    request: DownloadRequest,
    background_tasks: BackgroundTasks
):
    """
    Inicia descarga autom√°tica desde m√∫ltiples fuentes.
    
    - **query**: Cadena de b√∫squeda (ej: "generative artificial intelligence")
    - **sources**: Lista de bases de datos a consultar
    - **max_results_per_source**: Cantidad m√°xima de resultados por fuente
    - **export_format**: Formato de exportaci√≥n deseado
    """
    # Implementaci√≥n con Celery para tareas en background
    pass

@router.get("/status/{job_id}")
async def get_download_status(job_id: str):
    """Consulta el estado de una descarga en proceso."""
    pass

@router.get("/unified")
async def get_unified_data(
    format: Optional[str] = 'json',
    include_metadata: bool = True
):
    """Obtiene los datos unificados sin duplicados."""
    pass

@router.get("/duplicates")
async def get_duplicates_report():
    """Obtiene el reporte de publicaciones duplicadas eliminadas."""
    pass
```

#### 1.4. **Modelos de Datos**

```python
# Backend/app/models/publication.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import date

class Author(BaseModel):
    name: str
    affiliation: Optional[str] = None
    email: Optional[str] = None
    country: Optional[str] = None

class Publication(BaseModel):
    """Modelo unificado de publicaci√≥n cient√≠fica."""
    
    id: str = Field(description="ID √∫nico generado")
    title: str = Field(description="T√≠tulo del art√≠culo")
    abstract: str = Field(description="Resumen del art√≠culo")
    authors: List[Author] = Field(description="Lista de autores")
    keywords: List[str] = Field(default=[], description="Palabras clave")
    doi: Optional[str] = Field(None, description="DOI del art√≠culo")
    publication_date: Optional[date] = Field(None, description="Fecha de publicaci√≥n")
    journal: Optional[str] = Field(None, description="Revista o conferencia")
    source: str = Field(description="Fuente de datos (acm, sage, sciencedirect)")
    url: Optional[str] = Field(None, description="URL del art√≠culo")
    citation_count: Optional[int] = Field(0, description="N√∫mero de citas")
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "pub_001",
                "title": "Generative AI in Educational Contexts",
                "abstract": "This study explores...",
                "authors": [
                    {
                        "name": "John Doe",
                        "affiliation": "MIT",
                        "country": "USA"
                    }
                ],
                "keywords": ["Generative AI", "Education", "Machine Learning"],
                "doi": "10.1145/example",
                "publication_date": "2024-06-15",
                "journal": "ACM Transactions on Computer Education",
                "source": "acm"
            }
        }
```

#### 1.5. **Testing y Validaci√≥n**

**Archivos a crear:**
```
Backend/tests/
‚îú‚îÄ‚îÄ test_data_acquisition/
‚îÇ   ‚îú‚îÄ‚îÄ test_scrapers.py
‚îÇ   ‚îú‚îÄ‚îÄ test_deduplicator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_parsers.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api_endpoints.py
```

**Tests a implementar:**

```python
# test_deduplicator.py
import pytest
from app.services.data_acquisition.deduplicator import Deduplicator

def test_exact_duplicate_detection():
    """Verifica detecci√≥n de duplicados exactos."""
    publications = [
        {"title": "AI in Education", "doi": "10.1145/123"},
        {"title": "AI in Education", "doi": "10.1145/123"},  # Duplicado
    ]
    
    dedup = Deduplicator()
    unique, duplicates = dedup.deduplicate(publications)
    
    assert len(unique) == 1
    assert len(duplicates) == 1

def test_similar_title_detection():
    """Verifica detecci√≥n de duplicados por similitud."""
    publications = [
        {"title": "Generative AI in Education", "doi": "10.1145/123"},
        {"title": "Generative AI in Educational Contexts", "doi": "10.1145/456"},
    ]
    
    dedup = Deduplicator(similarity_threshold=0.8)
    unique, duplicates = dedup.deduplicate(publications)
    
    # Deber√≠a detectar como duplicados si similitud > 0.8
    assert len(unique) + len(duplicates) == 2
```

---

## üìù REQUERIMIENTO 2: ALGORITMOS DE SIMILITUD TEXTUAL

### **Objetivo**
Implementar 6 algoritmos de similitud textual (4 cl√°sicos + 2 con IA) con documentaci√≥n matem√°tica detallada y UI para comparaci√≥n de abstracts.

### **Componentes a Implementar**

#### 2.1. **Algoritmos Cl√°sicos de Similitud**

**Archivos a crear:**
```
Backend/app/services/ml_analysis/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ similarity/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_similarity.py        # Clase base abstracta
‚îÇ   ‚îú‚îÄ‚îÄ levenshtein.py            # Distancia de edici√≥n
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_cosine.py           # TF-IDF + Similitud del Coseno
‚îÇ   ‚îú‚îÄ‚îÄ jaccard.py                # Coeficiente de Jaccard
‚îÇ   ‚îú‚îÄ‚îÄ ngrams.py                 # Similitud por N-gramas
‚îÇ   ‚îú‚îÄ‚îÄ bert_embeddings.py        # BERT Embeddings
‚îÇ   ‚îú‚îÄ‚îÄ sentence_bert.py          # Sentence-BERT
‚îÇ   ‚îî‚îÄ‚îÄ similarity_analyzer.py    # Orquestador de an√°lisis
```

**2.1.1. Distancia de Levenshtein**

```python
# levenshtein.py
import numpy as np
from typing import Tuple, List, Dict

class LevenshteinSimilarity:
    """
    Implementaci√≥n de la Distancia de Levenshtein (Edit Distance).
    
    FUNDAMENTO MATEM√ÅTICO:
    =====================
    La distancia de Levenshtein entre dos cadenas s1 y s2 es el n√∫mero m√≠nimo
    de operaciones de edici√≥n (inserci√≥n, eliminaci√≥n, sustituci√≥n) necesarias
    para transformar s1 en s2.
    
    ALGORITMO DE PROGRAMACI√ìN DIN√ÅMICA:
    ===================================
    Sea DP[i][j] = distancia entre s1[0...i-1] y s2[0...j-1]
    
    Casos base:
    - DP[0][j] = j (insertar j caracteres)
    - DP[i][0] = i (eliminar i caracteres)
    
    Recurrencia:
    DP[i][j] = {
        DP[i-1][j-1]                           si s1[i-1] == s2[j-1]
        1 + min(DP[i-1][j],                    eliminaci√≥n
                DP[i][j-1],                    inserci√≥n
                DP[i-1][j-1])                  sustituci√≥n
                                               en caso contrario
    }
    
    COMPLEJIDAD:
    - Tiempo: O(m * n) donde m = len(s1), n = len(s2)
    - Espacio: O(m * n) para la matriz DP
    
    SIMILITUD NORMALIZADA:
    similarity = 1 - (distance / max(len(s1), len(s2)))
    """
    
    def __init__(self):
        self.name = "Distancia de Levenshtein"
        self.description = "Medida de similitud basada en operaciones de edici√≥n"
    
    def calculate_distance(
        self, 
        text1: str, 
        text2: str,
        return_matrix: bool = False
    ) -> Tuple[int, np.ndarray]:
        """
        Calcula la distancia de Levenshtein usando programaci√≥n din√°mica.
        
        Args:
            text1: Primer texto
            text2: Segundo texto
            return_matrix: Si True, retorna la matriz DP completa
        
        Returns:
            Tupla (distancia, matriz_dp)
        """
        m, n = len(text1), len(text2)
        
        # Inicializar matriz DP
        dp = np.zeros((m + 1, n + 1), dtype=int)
        
        # Casos base
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        # Llenar matriz usando programaci√≥n din√°mica
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if text1[i-1] == text2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(
                        dp[i-1][j],      # Eliminaci√≥n
                        dp[i][j-1],      # Inserci√≥n
                        dp[i-1][j-1]     # Sustituci√≥n
                    )
        
        distance = dp[m][n]
        return (distance, dp) if return_matrix else (distance, None)
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud normalizada [0, 1].
        
        similarity = 1 - (distance / max_length)
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (id√©nticos)
        """
        distance, _ = self.calculate_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1.0 - (distance / max_len)
        return similarity
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        An√°lisis detallado paso a paso con explicaci√≥n matem√°tica.
        
        Returns:
            Diccionario con:
            - distance: Distancia de Levenshtein
            - similarity: Similitud normalizada
            - dp_matrix: Matriz de programaci√≥n din√°mica
            - operations: Lista de operaciones necesarias
            - explanation: Explicaci√≥n paso a paso
        """
        distance, dp_matrix = self.calculate_distance(text1, text2, return_matrix=True)
        similarity = self.calculate_similarity(text1, text2)
        
        # Reconstruir secuencia de operaciones
        operations = self._reconstruct_operations(text1, text2, dp_matrix)
        
        # Generar explicaci√≥n detallada
        explanation = self._generate_explanation(
            text1, text2, distance, similarity, operations
        )
        
        return {
            "algorithm": self.name,
            "distance": distance,
            "similarity": similarity,
            "dp_matrix": dp_matrix.tolist(),
            "operations": operations,
            "explanation": explanation,
            "complexity": {
                "time": f"O({len(text1)} √ó {len(text2)}) = O({len(text1) * len(text2)})",
                "space": f"O({len(text1)} √ó {len(text2)}) = O({len(text1) * len(text2)})"
            }
        }
    
    def _reconstruct_operations(
        self, 
        text1: str, 
        text2: str, 
        dp: np.ndarray
    ) -> List[Dict[str, str]]:
        """Reconstruye la secuencia de operaciones de edici√≥n."""
        operations = []
        i, j = len(text1), len(text2)
        
        while i > 0 or j > 0:
            if i == 0:
                operations.append({
                    "type": "insert",
                    "char": text2[j-1],
                    "position": j-1
                })
                j -= 1
            elif j == 0:
                operations.append({
                    "type": "delete",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
            elif text1[i-1] == text2[j-1]:
                operations.append({
                    "type": "match",
                    "char": text1[i-1],
                    "position": i-1
                })
                i -= 1
                j -= 1
            else:
                # Determinar operaci√≥n √≥ptima
                delete_cost = dp[i-1][j]
                insert_cost = dp[i][j-1]
                substitute_cost = dp[i-1][j-1]
                
                min_cost = min(delete_cost, insert_cost, substitute_cost)
                
                if min_cost == substitute_cost:
                    operations.append({
                        "type": "substitute",
                        "from_char": text1[i-1],
                        "to_char": text2[j-1],
                        "position": i-1
                    })
                    i -= 1
                    j -= 1
                elif min_cost == delete_cost:
                    operations.append({
                        "type": "delete",
                        "char": text1[i-1],
                        "position": i-1
                    })
                    i -= 1
                else:
                    operations.append({
                        "type": "insert",
                        "char": text2[j-1],
                        "position": j-1
                    })
                    j -= 1
        
        return list(reversed(operations))
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        distance: int,
        similarity: float,
        operations: List[Dict]
    ) -> str:
        """Genera explicaci√≥n matem√°tica detallada."""
        explanation = f"""
        AN√ÅLISIS DE SIMILITUD CON DISTANCIA DE LEVENSHTEIN
        ==================================================
        
        Textos analizados:
        - Texto 1 (m={len(text1)}): "{text1[:50]}..."
        - Texto 2 (n={len(text2)}): "{text2[:50]}..."
        
        RESULTADOS:
        -----------
        - Distancia de Levenshtein: {distance}
        - Similitud normalizada: {similarity:.4f} ({similarity*100:.2f}%)
        
        OPERACIONES NECESARIAS ({len([op for op in operations if op['type'] != 'match'])} operaciones):
        ---------------------
        """
        
        for i, op in enumerate(operations[:10], 1):  # Primeras 10 operaciones
            if op['type'] == 'substitute':
                explanation += f"{i}. Sustituir '{op['from_char']}' por '{op['to_char']}' en posici√≥n {op['position']}\n"
            elif op['type'] == 'insert':
                explanation += f"{i}. Insertar '{op['char']}' en posici√≥n {op['position']}\n"
            elif op['type'] == 'delete':
                explanation += f"{i}. Eliminar '{op['char']}' de posici√≥n {op['position']}\n"
        
        if len(operations) > 10:
            explanation += f"... ({len(operations) - 10} operaciones m√°s)\n"
        
        return explanation
```

**2.1.2. TF-IDF + Similitud del Coseno**

```python
# tfidf_cosine.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict, List, Tuple

class TFIDFCosineSimilarity:
    """
    Implementaci√≥n de Similitud TF-IDF + Coseno.
    
    FUNDAMENTO MATEM√ÅTICO:
    =====================
    
    1. TF-IDF (Term Frequency - Inverse Document Frequency):
       
       TF(t,d) = Frecuencia del t√©rmino t en el documento d
                 -----------------------------------------------
                 N√∫mero total de t√©rminos en el documento d
       
       IDF(t,D) = log( N√∫mero total de documentos en D )
                      ------------------------------------
                      N√∫mero de documentos que contienen t
       
       TF-IDF(t,d,D) = TF(t,d) √ó IDF(t,D)
    
    2. Similitud del Coseno:
       
       cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
       
       Donde:
       - A ¬∑ B = Producto punto de vectores A y B = Œ£(Ai √ó Bi)
       - ||A|| = Norma euclidiana de A = ‚àö(Œ£(Ai¬≤))
       - ||B|| = Norma euclidiana de B = ‚àö(Œ£(Bi¬≤))
       
       Rango: [-1, 1]
       - 1 = Vectores id√©nticos en direcci√≥n
       - 0 = Vectores ortogonales
       - -1 = Vectores opuestos
    
    INTERPRETACI√ìN:
    - La similitud del coseno mide el √°ngulo entre dos vectores en el espacio
      de caracter√≠sticas TF-IDF.
    - No considera la magnitud, solo la orientaci√≥n.
    - Ideal para comparar documentos de diferentes longitudes.
    
    COMPLEJIDAD:
    - Vectorizaci√≥n TF-IDF: O(n √ó m) donde n = docs, m = vocabulario
    - Similitud del coseno: O(m) por par de documentos
    """
    
    def __init__(
        self, 
        max_features: int = 5000,
        ngram_range: Tuple[int, int] = (1, 2),
        min_df: int = 1,
        max_df: float = 0.9
    ):
        """
        Inicializa el analizador TF-IDF + Coseno.
        
        Args:
            max_features: N√∫mero m√°ximo de caracter√≠sticas (t√©rminos)
            ngram_range: Rango de n-gramas a considerar
            min_df: Frecuencia m√≠nima de documento
            max_df: Frecuencia m√°xima de documento (fracci√≥n)
        """
        self.name = "TF-IDF + Similitud del Coseno"
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=min_df,
            max_df=max_df,
            stop_words='english',
            lowercase=True,
            strip_accents='unicode'
        )
    
    def calculate_similarity(
        self, 
        text1: str, 
        text2: str
    ) -> float:
        """
        Calcula similitud del coseno entre dos textos.
        
        Returns:
            float entre 0 (completamente diferentes) y 1 (id√©nticos)
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return float(similarity)
    
    def analyze_step_by_step(
        self, 
        text1: str, 
        text2: str
    ) -> Dict[str, any]:
        """
        An√°lisis detallado con vectores TF-IDF y explicaci√≥n matem√°tica.
        
        Returns:
            Diccionario con an√°lisis completo paso a paso
        """
        # Vectorizar textos
        tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
        
        # Obtener nombres de caracter√≠sticas
        feature_names = self.vectorizer.get_feature_names_out()
        
        # Extraer vectores TF-IDF
        vector1 = tfidf_matrix[0].toarray()[0]
        vector2 = tfidf_matrix[1].toarray()[0]
        
        # Calcular similitud del coseno
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        # Identificar t√©rminos m√°s relevantes
        top_terms_text1 = self._get_top_terms(vector1, feature_names, top_n=10)
        top_terms_text2 = self._get_top_terms(vector2, feature_names, top_n=10)
        
        # Calcular normas euclidianas
        norm1 = np.linalg.norm(vector1)
        norm2 = np.linalg.norm(vector2)
        
        # Calcular producto punto
        dot_product = np.dot(vector1, vector2)
        
        # Generar explicaci√≥n detallada
        explanation = self._generate_explanation(
            text1, text2, vector1, vector2, 
            dot_product, norm1, norm2, similarity,
            top_terms_text1, top_terms_text2
        )
        
        return {
            "algorithm": self.name,
            "similarity": float(similarity),
            "vectors": {
                "text1": {
                    "vector": vector1.tolist(),
                    "norm": float(norm1),
                    "top_terms": top_terms_text1
                },
                "text2": {
                    "vector": vector2.tolist(),
                    "norm": float(norm2),
                    "top_terms": top_terms_text2
                }
            },
            "dot_product": float(dot_product),
            "cosine_angle_degrees": float(np.degrees(np.arccos(similarity))),
            "vocabulary_size": len(feature_names),
            "explanation": explanation,
            "complexity": {
                "vectorization": f"O(n √ó m) donde n=2 documentos, m={len(feature_names)} t√©rminos",
                "similarity": "O(m) para producto punto y normas"
            }
        }
    
    def _get_top_terms(
        self, 
        vector: np.ndarray, 
        feature_names: np.ndarray, 
        top_n: int = 10
    ) -> List[Dict[str, any]]:
        """Extrae los t√©rminos con mayor peso TF-IDF."""
        # Obtener √≠ndices de t√©rminos ordenados por peso TF-IDF
        top_indices = np.argsort(vector)[-top_n:][::-1]
        
        top_terms = []
        for idx in top_indices:
            if vector[idx] > 0:
                top_terms.append({
                    "term": feature_names[idx],
                    "tfidf_weight": float(vector[idx])
                })
        
        return top_terms
    
    def _generate_explanation(
        self,
        text1: str,
        text2: str,
        vector1: np.ndarray,
        vector2: np.ndarray,
        dot_product: float,
        norm1: float,
        norm2: float,
        similarity: float,
        top_terms1: List[Dict],
        top_terms2: List[Dict]
    ) -> str:
        """Genera explicaci√≥n matem√°tica detallada."""
        angle_degrees = np.degrees(np.arccos(np.clip(similarity, -1, 1)))
        
        explanation = f"""
        AN√ÅLISIS DE SIMILITUD CON TF-IDF + COSENO
        =========================================
        
        Textos analizados:
        - Texto 1: "{text1[:50]}..."
        - Texto 2: "{text2[:50]}..."
        
        PASO 1: VECTORIZACI√ìN TF-IDF
        -----------------------------
        Vocabulario total: {len(vector1)} t√©rminos √∫nicos
        
        T√©rminos m√°s relevantes en Texto 1:
        """
        
        for i, term_info in enumerate(top_terms1[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += "\nT√©rminos m√°s relevantes en Texto 2:\n"
        for i, term_info in enumerate(top_terms2[:5], 1):
            explanation += f"  {i}. '{term_info['term']}': TF-IDF = {term_info['tfidf_weight']:.4f}\n"
        
        explanation += f"""
        
        PASO 2: C√ÅLCULO DE SIMILITUD DEL COSENO
        ----------------------------------------
        F√≥rmula: cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
        
        Producto punto (A ¬∑ B): {dot_product:.6f}
        Norma de A (||A||): {norm1:.6f}
        Norma de B (||B||): {norm2:.6f}
        
        Similitud del coseno: {dot_product:.6f} / ({norm1:.6f} √ó {norm2:.6f})
                            = {dot_product:.6f} / {norm1 * norm2:.6f}
                            = {similarity:.6f}
        
        √Ångulo entre vectores: {angle_degrees:.2f}¬∞
        
        INTERPRETACI√ìN:
        ---------------
        - Similitud: {similarity:.4f} ({similarity*100:.2f}%)
        - √Ångulo: {angle_degrees:.2f}¬∞ (0¬∞ = id√©nticos, 90¬∞ = ortogonales)
        - Los documentos son {"muy similares" if similarity > 0.8 else "moderadamente similares" if similarity > 0.5 else "poco similares"}
        """
        
        return explanation
```

**(Continuar√© con los otros 4 algoritmos de similitud en el siguiente mensaje debido a la longitud...)**

Ahora voy a crear el resto de los algoritmos de similitud y los siguientes requerimientos de manera estructurada.

---

## üì¶ PR√ìXIMOS PASOS A IMPLEMENTAR

Ahora voy a crear la estructura completa del proyecto con todos los archivos necesarios. Comenzar√© implementando los componentes paso a paso.

¬øDeseas que contin√∫e con:

1. **Implementaci√≥n completa del Requerimiento 1** (Scrapers + Deduplicaci√≥n)
2. **Completar todos los algoritmos de similitud** (6 algoritmos completos)
3. **Implementaci√≥n del Requerimiento 3** (Frecuencias de conceptos)
4. **Implementaci√≥n del Requerimiento 4** (Clustering y dendrogramas)
5. **O prefieres que cree primero la estructura completa** y luego implementemos cada m√≥dulo?

Tambi√©n puedo:
- Crear scripts de inicializaci√≥n de base de datos
- Configurar Docker y Docker Compose
- Implementar el frontend con los componentes de visualizaci√≥n
- Crear tests automatizados

**¬øPor d√≥nde quieres que empiece?** Te recomiendo seguir el orden de los requerimientos para tener un flujo de trabajo l√≥gico.
